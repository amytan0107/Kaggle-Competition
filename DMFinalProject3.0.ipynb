{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### change from previous:\n\n1. use progress report 2 method\n   \n2. no autoencoder, no feature engineer. don't handle missing numerical values. models = lightgbm, xgboost & catboost which auto handles.\n\n3. no autoencoder, no feature engineer. use imputer = SimpleImputer(strategy='median') in models.","metadata":{}},{"cell_type":"markdown","source":"## Progress Report 2 Method","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nfrom IPython.display import clear_output\nfrom colorama import Fore, Style, init\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import KNNImputer, IterativeImputer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import (RandomForestRegressor, VotingRegressor, GradientBoostingRegressor)\nfrom sklearn.metrics import (classification_report, confusion_matrix, mean_squared_error,\n                             accuracy_score, cohen_kappa_score, r2_score, make_scorer)\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeRegressor \nfrom scipy.optimize import minimize\nfrom sklearn.base import clone\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:35:10.076737Z","iopub.execute_input":"2024-11-15T11:35:10.077352Z","iopub.status.idle":"2024-11-15T11:35:10.089198Z","shell.execute_reply.started":"2024-11-15T11:35:10.077302Z","shell.execute_reply":"2024-11-15T11:35:10.087548Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Feature Engineering\ndef feature_engineering(df):\n    # Calculate new features\n    new_features = pd.DataFrame({\n        'BMI_Age': df['Physical-BMI'] * df['Basic_Demos-Age'],\n        'Internet_Hours_Age': df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age'],\n        'BMI_Internet_Hours': df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday'],\n        'BFP_BMI': df['BIA-BIA_Fat'] / df['BIA-BIA_BMI'],\n        'FFMI_BFP': df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat'],\n        'FMI_BFP': df['BIA-BIA_FMI'] / df['BIA-BIA_Fat'],\n        'LST_TBW': df['BIA-BIA_LST'] / df['BIA-BIA_TBW'],\n        'BFP_BMR': df['BIA-BIA_Fat'] * df['BIA-BIA_BMR'],\n        'BFP_DEE': df['BIA-BIA_Fat'] * df['BIA-BIA_DEE'],\n        'BMR_Weight': df['BIA-BIA_BMR'] / df['Physical-Weight'],\n        'DEE_Weight': df['BIA-BIA_DEE'] / df['Physical-Weight'],\n        'SMM_Height': df['BIA-BIA_SMM'] / df['Physical-Height'],\n        'Muscle_to_Fat': df['BIA-BIA_SMM'] / df['BIA-BIA_FMI'],\n        'Hydration_Status': df['BIA-BIA_TBW'] / df['Physical-Weight'],\n        'ICW_TBW': df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    })\n\n    # Concatenate new features with the original DataFrame\n    df = pd.concat([df, new_features], axis=1)\n    \n    return df\n\ndef load_and_process_data(directory):\n    files = os.listdir(directory)\n    all_stats = []\n\n    with ThreadPoolExecutor() as executor:\n        futures = [executor.submit(pd.read_parquet, os.path.join(directory, file, 'part-0.parquet')) for file in files]\n        for future in tqdm(futures):\n            data = future.result()\n            if 'step' in data.columns:\n                data.drop('step', axis=1, inplace=True)\n\n            # Calculate summary statistics\n            stats = data.describe().values.reshape(-1)\n            all_stats.append(stats)\n\n    # Create a DataFrame for summary statistics\n    stat_columns = [f\"stat_{i}\" for i in range(len(all_stats[0]))]\n    summary_df = pd.DataFrame(all_stats, columns=stat_columns)\n    summary_df['id'] = [file.split('=')[1] for file in files]  # Extract 'id' from filenames\n\n    return summary_df\n\nclass SimpleAutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(SimpleAutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim * 2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, encoding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim * 2, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.decoder(self.encoder(x))\n\ndef train_autoencoder(data, encoding_dim=10, epochs=20, batch_size=16):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n    tensor_data = torch.FloatTensor(scaled_data)\n\n    autoencoder = SimpleAutoEncoder(input_dim=tensor_data.shape[1], encoding_dim=encoding_dim)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n\n    for epoch in range(epochs):\n        for i in range(0, len(tensor_data), batch_size):\n            batch = tensor_data[i:i + batch_size]\n            optimizer.zero_grad()\n            loss = criterion(autoencoder(batch), batch)\n            loss.backward()\n            optimizer.step()\n        if (epoch + 1) % 5 == 0:\n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n\n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(tensor_data).numpy()\n    \n    return pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n    \ndef impute_missing_values(data, season_columns, season_mapping):\n    # Encode Seasons\n    data[season_columns] = data[season_columns].map(lambda x: season_mapping.get(x, x))\n    \n    # Identify numeric columns\n    numeric_cols = data.select_dtypes(include=['float64', 'float32', 'int64']).columns\n    \n    # Scale numeric features for KNN imputation\n    scaler = StandardScaler()\n    data_scaled = data.copy()\n    data_scaled[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n    \n    # Initialize the imputer and apply it only on numeric columns with missing values\n    imputer = KNNImputer(n_neighbors=5)\n    imputed_numeric_data = imputer.fit_transform(data_scaled[numeric_cols])\n    imputed_scaled_df = pd.DataFrame(imputed_numeric_data, columns=numeric_cols)\n    \n    # Invert scaling to original scale for imputed numeric columns\n    imputed_data = data.copy()\n    imputed_data[numeric_cols] = scaler.inverse_transform(imputed_scaled_df)\n    \n    # Clip and convert 'sii' to integers\n    if 'sii' in imputed_data.columns:\n        imputed_data['sii'] = imputed_data['sii'].clip(lower=0, upper=3).round().astype(int)\n    \n    # Ensure other columns remain intact\n    for col in imputed_data.columns:\n        if col not in numeric_cols:\n            imputed_data[col] = data[col]\n    \n    # Convert season columns to integers\n    imputed_data[season_columns] = imputed_data[season_columns].clip(lower=1, upper=4).round().astype(int)\n    \n    return imputed_data\n\n# Define QWK calculation function\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n# Function to apply threshold-based rounding to predictions\ndef threshold_rounder(predictions, thresholds):\n    return np.where(predictions < thresholds[0], 0,\n                    np.where(predictions < thresholds[1], 1,\n                             np.where(predictions < thresholds[2], 2, 3)))\n\n# Threshold optimization to maximize QWK\ndef optimize_qwk_thresholds(predictions, y_true):\n    def evaluate_thresholds(thresholds):\n        rounded_preds = threshold_rounder(predictions, thresholds)\n        return -quadratic_weighted_kappa(y_true, rounded_preds)\n    \n    # Optimize using the Nelder-Mead method\n    result = minimize(evaluate_thresholds, x0=[0.5, 1.5, 2.5], method='Nelder-Mead')\n    return result.x if result.success else [0.5, 1.5, 2.5]\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:23:56.843523Z","iopub.execute_input":"2024-11-15T11:23:56.844330Z","iopub.status.idle":"2024-11-15T11:23:56.877530Z","shell.execute_reply.started":"2024-11-15T11:23:56.844280Z","shell.execute_reply":"2024-11-15T11:23:56.875960Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"n_splits = 5\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:23:56.879270Z","iopub.execute_input":"2024-11-15T11:23:56.879661Z","iopub.status.idle":"2024-11-15T11:23:56.896697Z","shell.execute_reply.started":"2024-11-15T11:23:56.879621Z","shell.execute_reply":"2024-11-15T11:23:56.895219Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Model\ndef train_and_evaluate(train, test, model):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    train_S = []\n    test_S = []\n    \n    # Arrays to store out-of-fold predictions and test predictions\n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test), n_splits))\n    \n    # Set up cross-validation\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, test_idx) in enumerate(tqdm(skf.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        print(f\"Training fold {fold + 1}/{n_splits}...\")\n        \n        # Split the data into training and validation sets\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n        \n        # Standardize the features using StandardScaler\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        test_scaled = scaler.transform(test)\n        \n        # Fit the model on the scaled training data\n        model = clone(model)\n        model.fit(X_train_scaled, y_train)\n\n        y_train_pred = model.predict(X_train_scaled)\n        y_val_pred = model.predict(X_val_scaled)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n\n        test_preds[:, fold] = model.predict(test)\n\n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK: {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK: {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n\n    oof_tuned = threshold_rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"Optimized QWK SCORE: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_rounder(tpm, KappaOPtimizer.x)\n\n    # Prepare submission DataFrame\n    submission = pd.DataFrame({\n        'id': sample_submission['id'],  # Use 'id' from sample submission\n        'sii': tpTuned\n    })\n    \n    return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:55:08.607895Z","iopub.execute_input":"2024-11-15T11:55:08.608576Z","iopub.status.idle":"2024-11-15T11:55:08.624989Z","shell.execute_reply.started":"2024-11-15T11:55:08.608527Z","shell.execute_reply":"2024-11-15T11:55:08.623580Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Load Data\n\ntrain_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# Load actigraphy time series data\ntrain_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\n# Autoencode Data\ntrain_ts_encoded = train_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\ntest_ts_encoded = train_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\ntime_series_cols = train_ts_encoded.columns.tolist()\n# Add 'id' back to the encoded DataFrame\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\n# Merge Data\ntrain = pd.merge(train_og, train_ts_encoded, how=\"left\", on='id')\ntest = pd.merge(test_og, test_ts_encoded, how=\"left\", on='id')\n\n# Impute Missing Data\n# Define season mapping\nseason_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\n# For the train set\nseason_columns_train = [col for col in train.columns if 'Season' in col]\ntrain_imputed = impute_missing_values(train, season_columns_train, season_mapping)\n# For the test set\nseason_columns_test = [col for col in test.columns if 'Season' in col]\ntest_imputed = impute_missing_values(test, season_columns_test, season_mapping)\n\n# Perform feature engineering\ntrain_imputed = feature_engineering(train_imputed)\ntrain_imputed.dropna(thresh=1, axis=0, inplace=True)\ntrain_imputed.replace([np.inf, -np.inf], 0, inplace=True)\ntest_imputed = feature_engineering(test_imputed)\n\n# Get the columns from both DataFrames\ntrain_cols = set(train_og.columns)\ntest_cols = set(test_og.columns)\n\n# Find common columns\ncommon_cols = train_cols.intersection(test_cols)\nfeaturesCols = [col for col in common_cols if col != 'id']\nfeaturesCols += time_series_cols\n\ntest_imputed = test_imputed[featuresCols]\nfeaturesCols.append('sii')\ntrain_imputed = train_imputed[featuresCols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:24:02.873491Z","iopub.execute_input":"2024-11-15T11:24:02.873973Z","iopub.status.idle":"2024-11-15T11:27:05.972692Z","shell.execute_reply.started":"2024-11-15T11:24:02.873926Z","shell.execute_reply":"2024-11-15T11:27:05.971391Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 996/996 [02:35<00:00,  6.39it/s]\n100%|██████████| 2/2 [00:00<00:00,  9.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/100, Loss: 1.4515\nEpoch 10/100, Loss: 1.4007\nEpoch 15/100, Loss: 1.3826\nEpoch 20/100, Loss: 1.3762\nEpoch 25/100, Loss: 1.3576\nEpoch 30/100, Loss: 1.3525\nEpoch 35/100, Loss: 1.3503\nEpoch 40/100, Loss: 1.3513\nEpoch 45/100, Loss: 1.3492\nEpoch 50/100, Loss: 1.3504\nEpoch 55/100, Loss: 1.3495\nEpoch 60/100, Loss: 1.3497\nEpoch 65/100, Loss: 1.3534\nEpoch 70/100, Loss: 1.3511\nEpoch 75/100, Loss: 1.3490\nEpoch 80/100, Loss: 1.3432\nEpoch 85/100, Loss: 1.3314\nEpoch 90/100, Loss: 1.3269\nEpoch 95/100, Loss: 1.3286\nEpoch 100/100, Loss: 1.3272\nEpoch 5/100, Loss: 1.0802\nEpoch 10/100, Loss: 1.0235\nEpoch 15/100, Loss: 0.8995\nEpoch 20/100, Loss: 0.7003\nEpoch 25/100, Loss: 0.5164\nEpoch 30/100, Loss: 0.4415\nEpoch 35/100, Loss: 0.4284\nEpoch 40/100, Loss: 0.4272\nEpoch 45/100, Loss: 0.4271\nEpoch 50/100, Loss: 0.4271\nEpoch 55/100, Loss: 0.4271\nEpoch 60/100, Loss: 0.4271\nEpoch 65/100, Loss: 0.4271\nEpoch 70/100, Loss: 0.4271\nEpoch 75/100, Loss: 0.4271\nEpoch 80/100, Loss: 0.4271\nEpoch 85/100, Loss: 0.4271\nEpoch 90/100, Loss: 0.4271\nEpoch 95/100, Loss: 0.4271\nEpoch 100/100, Loss: 0.4271\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Model hyperparameters\nxgb_params = {\n    'n_estimators': 200,\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'gamma': 0,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': 42\n}\ncat_params = {\n    'iterations': 200,\n    'learning_rate': 0.05,\n    'depth': 6,\n    'l2_leaf_reg': 10,\n    'subsample': 0.8,\n    'rsm': 0.8,\n    'border_count': 32,\n    'random_state': 42,\n    'silent': True\n}\nrf_params = {\n    'n_estimators': 200,\n    'max_depth': None,\n    'min_samples_split': 2,\n    'min_samples_leaf': 1,\n    'bootstrap': True,\n    'random_state': 42\n}\ngb_params = {\n    'n_estimators': 200,\n    'learning_rate': 0.05,\n    'max_depth': 3,\n    'min_samples_split': 2,\n    'min_samples_leaf': 1,\n    'subsample': 1.0,\n    'random_state': 42\n}\n\n# Initialize the models\nrf_model = RandomForestRegressor(**rf_params)\nxgb_model = XGBRegressor(**xgb_params)\ncat_model = CatBoostRegressor(**cat_params)\ngb_model = GradientBoostingRegressor(**gb_params)\n\n# Initialize the voting regressor ensemble\nvoting_regressor = VotingRegressor(estimators=[\n    ('rf', rf_model),\n    ('xgboost', xgb_model),\n    ('catboost', cat_model),\n    ('gb', gb_model)\n])\n\nsubmission1 = train_and_evaluate(train_imputed, test_imputed, voting_regressor)\n\n# Display the submission DataFrame\nsubmission1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:55:13.567297Z","iopub.execute_input":"2024-11-15T11:55:13.567864Z","iopub.status.idle":"2024-11-15T12:01:13.417423Z","shell.execute_reply.started":"2024-11-15T11:55:13.567727Z","shell.execute_reply":"2024-11-15T12:01:13.416129Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [05:59<00:00, 71.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK: 0.8488\nMean Validation QWK: 0.4886\nOptimized QWK SCORE: \u001b[36m\u001b[1m 0.559\u001b[0m\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"          id  sii\n0   00008ff9    2\n1   000fd460    2\n2   00105258    2\n3   00115b9f    2\n4   0016bb22    2\n5   001f3379    1\n6   0038ba98    2\n7   0068a485    2\n8   0069fbed    2\n9   0083e397    2\n10  0087dd65    2\n11  00abe655    2\n12  00ae59c9    2\n13  00af6387    2\n14  00bd4359    2\n15  00c0cd71    2\n16  00d56d4b    2\n17  00d9913d    2\n18  00e6167c    2\n19  00ebc35d    2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## No autoencoder. Don't handle missing numerical values. Models = lightgbm, xgboost & catboost","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(train, test, model):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test)\n\n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK: {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK: {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"Optimized QWK SCORE: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample_submission['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n    \n# Load Data\n\ntrain_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# Load actigraphy time series data\ntrain_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train_og, train_ts, how=\"left\", on='id')\ntest = pd.merge(test_og, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1) \n\n# Get the columns from both DataFrames\ntrain_cols = set(train_og.columns)\ntest_cols = set(test_og.columns)\n\n# Find common columns\ncommon_cols = train_cols.intersection(test_cols)\nfeaturesCols = [col for col in common_cols if col != 'id']\nfeaturesCols += time_series_cols\n\ntest = test[featuresCols]\n\nfeaturesCols.append('sii')\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\n# Model hyperparameters (keeping your original hyperparameters)\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': 42\n}\n\n# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01  # Increased from 2.68e-06\n}\n\n# Model parameters for CatBoost\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': 42,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\n# Initialize the models\nLight = LGBMRegressor(**Params, random_state=42, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Initialize the voting regressor ensemble\nvoting_regressor = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\nsubmission2 = train_and_evaluate(train, test, voting_regressor)\n\n# Display the submission DataFrame\nsubmission2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:27:30.659221Z","iopub.execute_input":"2024-11-15T12:27:30.660340Z","iopub.status.idle":"2024-11-15T12:31:28.331867Z","shell.execute_reply.started":"2024-11-15T12:27:30.660245Z","shell.execute_reply":"2024-11-15T12:31:28.329301Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [01:31<00:00, 18.30s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK: 0.7586\nMean Validation QWK: 0.3985\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Optimized QWK SCORE: \u001b[36m\u001b[1m 0.464\u001b[0m\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"          id  sii\n0   00008ff9    1\n1   000fd460    0\n2   00105258    0\n3   00115b9f    0\n4   0016bb22    1\n5   001f3379    1\n6   0038ba98    0\n7   0068a485    0\n8   0069fbed    1\n9   0083e397    1\n10  0087dd65    0\n11  00abe655    1\n12  00ae59c9    1\n13  00af6387    1\n14  00bd4359    1\n15  00c0cd71    1\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    0\n19  00ebc35d    1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## No autoencoder. Use imputer = SimpleImputer(strategy='median'). Use more models.","metadata":{}},{"cell_type":"code","source":"# Load Data\ntrain_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# Load actigraphy time series data\ntrain_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train_og, train_ts, how=\"left\", on='id')\ntest = pd.merge(test_og, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1) \n\n# Get the columns from both DataFrames\ntrain_cols = set(train_og.columns)\ntest_cols = set(test_og.columns)\n\n# Find common columns\ncommon_cols = train_cols.intersection(test_cols)\nfeaturesCols = [col for col in common_cols if col != 'id']\nfeaturesCols += time_series_cols\n\ntest = test[featuresCols]\n\nfeaturesCols.append('sii')\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n        \ntrain = update(train)\ntest = update(test)\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n    \nimputer = SimpleImputer(strategy='median')\n\nensemble = VotingRegressor(estimators=[\n    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n])\n\nsubmission3 = train_and_evaluate(train, test, ensemble)\n\n# Display the submission DataFrame\nsubmission3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:36:12.819697Z","iopub.execute_input":"2024-11-15T12:36:12.820280Z","iopub.status.idle":"2024-11-15T12:41:15.534658Z","shell.execute_reply.started":"2024-11-15T12:36:12.820230Z","shell.execute_reply":"2024-11-15T12:41:15.533212Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [02:38<00:00, 31.63s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK: 0.9173\nMean Validation QWK: 0.3817\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Optimized QWK SCORE: \u001b[36m\u001b[1m 0.445\u001b[0m\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"          id  sii\n0   00008ff9    2\n1   000fd460    0\n2   00105258    0\n3   00115b9f    0\n4   0016bb22    0\n5   001f3379    1\n6   0038ba98    0\n7   0068a485    0\n8   0069fbed    2\n9   0083e397    0\n10  0087dd65    1\n11  00abe655    0\n12  00ae59c9    2\n13  00af6387    2\n14  00bd4359    2\n15  00c0cd71    2\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    0\n19  00ebc35d    1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### TPOT to find best parameters. Stacked Regressor.","metadata":{}},{"cell_type":"code","source":"# Load Data\ntrain_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# Load actigraphy time series data\ntrain_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\n# Autoencode Data\ntrain_ts_encoded = train_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\ntest_ts_encoded = train_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\ntime_series_cols = train_ts_encoded.columns.tolist()\n\n# Add 'id' back to the encoded DataFrame\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\n# Merge Data\ntrain = pd.merge(train_og, train_ts_encoded, how=\"left\", on='id')\ntest = pd.merge(test_og, test_ts_encoded, how=\"left\", on='id')\n\n# Impute Missing Data\n# Define season mapping\nseason_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\n\n# For the train set\nseason_columns_train = [col for col in train.columns if 'Season' in col]\ntrain_imputed = impute_missing_values(train, season_columns_train, season_mapping)\n\n# For the test set\nseason_columns_test = [col for col in test.columns if 'Season' in col]\ntest_imputed = impute_missing_values(test, season_columns_test, season_mapping)\n\n# Perform feature engineering\ntrain_imputed = feature_engineering(train_imputed)\ntrain_imputed.dropna(thresh=1, axis=0, inplace=True)\ntrain_imputed.replace([np.inf, -np.inf], 0, inplace=True)\ntest_imputed = feature_engineering(test_imputed)\n\n# Get the columns from both DataFrames\ntrain_cols = set(train_og.columns)\ntest_cols = set(test_og.columns)\n\n# Find common columns\ncommon_cols = train_cols.intersection(test_cols)\nfeaturesCols = [col for col in common_cols if col != 'id']\nfeaturesCols += time_series_cols\n\ntest_imputed = test_imputed[featuresCols]\nfeaturesCols.append('sii')\ntrain_imputed = train_imputed[featuresCols]\n\ndef train_and_evaluate(train, test, models, use_stacking=False, final_estimator_class=None):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test), n_splits))\n\n    # Set up cross-validation\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    for fold, (train_idx, test_idx) in enumerate(tqdm(skf.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        print(f\"Training fold {fold + 1}/{n_splits}...\")\n\n        # Split the data into training and validation sets\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        # Standardize the features using StandardScaler\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        test_scaled = scaler.transform(test)\n\n        if use_stacking:\n            # Initialize the stacked regressor with the specified final estimator\n            stacked_regressor = StackingRegressor(\n                estimators=[(model_name, model_class()) for model_class, model_name in models],\n                final_estimator=final_estimator_class(),\n                n_jobs=-1\n            )\n            model = stacked_regressor\n\n        else:\n            # Initialize the voting regressor with the specified models\n            voting_regressor = VotingRegressor(\n                estimators=[(model_name, model_class()) for model_class, model_name in models]\n            )\n            model = voting_regressor\n\n        # Fit the model on the scaled training data\n        model = clone(model)\n        model.fit(X_train_scaled, y_train)\n\n        # Generate predictions for the validation\n        y_train_pred = model.predict(X_train_scaled)\n        y_val_pred = model.predict(X_val_scaled)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test)\n\n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK ---> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")  \n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_rounder(tpm, KappaOPtimizer.x)\n\n    # Prepare submission DataFrame\n    submission = pd.DataFrame({\n        'id': sample_submission['id'],  # Use 'id' from sample submission\n        'sii': tpTuned\n    })\n\n    return submission\n\nmodels = [\n    (lambda: RandomForestRegressor(), 'Random Forest'),\n    (lambda: XGBRegressor(verbosity=0), 'XGBoost'),\n    (lambda: CatBoostRegressor(verbose=0), 'CatBoost'),\n    (lambda: GradientBoostingRegressor(), 'Gradient Boosting')\n]\n\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Stacking Regressor\n\nsubmission4 = train_and_evaluate(\n    train_imputed, \n    test_imputed, \n    models, \n    use_stacking=True,\n    final_estimator_class=GradientBoostingRegressor\n)\n\nsubmission4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:02:58.406804Z","iopub.execute_input":"2024-11-15T13:02:58.407307Z"}},"outputs":[{"name":"stderr","text":" 74%|███████▍  | 737/996 [01:51<00:35,  7.32it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"sub1 = submission1\nsub2 = submission2\nsub3 = submission3\nsub4 = submission4\n\nsub1 = sub1.sort_values(by='id').reset_index(drop=True)\nsub2 = sub2.sort_values(by='id').reset_index(drop=True)\nsub3 = sub3.sort_values(by='id').reset_index(drop=True)\nsub4 = sub4.sort_values(by='id').reset_index(drop=True)\n\ncombined = pd.DataFrame({\n    'id': sub1['id'],\n    'sii_1': sub1['sii'],\n    'sii_2': sub2['sii'],\n    'sii_3': sub3['sii'],\n    'sii_4': sub4['sii']\n})\n\ndef majority_vote(row):\n    return row.mode()[0]\n\ncombined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3', 'sii_4']].apply(majority_vote, axis=1)\n\nfinal_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Majority voting completed and saved to 'Submission.csv'\")\nfinal_submission","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eec9863",
   "metadata": {
    "papermill": {
     "duration": 0.007044,
     "end_time": "2024-11-15T13:10:33.935273",
     "exception": false,
     "start_time": "2024-11-15T13:10:33.928229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### change from previous:\n",
    "\n",
    "1. use progress report 2 method\n",
    "   \n",
    "2. no autoencoder, no feature engineer. don't handle missing numerical values. models = lightgbm, xgboost & catboost which auto handles.\n",
    "\n",
    "3. no autoencoder, no feature engineer. use imputer = SimpleImputer(strategy='median') in models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da61e5e",
   "metadata": {
    "papermill": {
     "duration": 0.006321,
     "end_time": "2024-11-15T13:10:33.948202",
     "exception": false,
     "start_time": "2024-11-15T13:10:33.941881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Progress Report 2 Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568a94b8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-15T13:10:33.964019Z",
     "iopub.status.busy": "2024-11-15T13:10:33.962926Z",
     "iopub.status.idle": "2024-11-15T13:10:59.088260Z",
     "shell.execute_reply": "2024-11-15T13:10:59.086986Z"
    },
    "papermill": {
     "duration": 25.136665,
     "end_time": "2024-11-15T13:10:59.091279",
     "exception": false,
     "start_time": "2024-11-15T13:10:33.954614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "from colorama import Fore, Style, init\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (RandomForestRegressor, VotingRegressor, GradientBoostingRegressor)\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, mean_squared_error,\n",
    "                             accuracy_score, cohen_kappa_score, r2_score, make_scorer)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import clone\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9add4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:10:59.106911Z",
     "iopub.status.busy": "2024-11-15T13:10:59.106045Z",
     "iopub.status.idle": "2024-11-15T13:10:59.144169Z",
     "shell.execute_reply": "2024-11-15T13:10:59.142639Z"
    },
    "papermill": {
     "duration": 0.049271,
     "end_time": "2024-11-15T13:10:59.147053",
     "exception": false,
     "start_time": "2024-11-15T13:10:59.097782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def feature_engineering(df):\n",
    "    # Calculate new features\n",
    "    new_features = pd.DataFrame({\n",
    "        'BMI_Age': df['Physical-BMI'] * df['Basic_Demos-Age'],\n",
    "        'Internet_Hours_Age': df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age'],\n",
    "        'BMI_Internet_Hours': df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday'],\n",
    "        'BFP_BMI': df['BIA-BIA_Fat'] / df['BIA-BIA_BMI'],\n",
    "        'FFMI_BFP': df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat'],\n",
    "        'FMI_BFP': df['BIA-BIA_FMI'] / df['BIA-BIA_Fat'],\n",
    "        'LST_TBW': df['BIA-BIA_LST'] / df['BIA-BIA_TBW'],\n",
    "        'BFP_BMR': df['BIA-BIA_Fat'] * df['BIA-BIA_BMR'],\n",
    "        'BFP_DEE': df['BIA-BIA_Fat'] * df['BIA-BIA_DEE'],\n",
    "        'BMR_Weight': df['BIA-BIA_BMR'] / df['Physical-Weight'],\n",
    "        'DEE_Weight': df['BIA-BIA_DEE'] / df['Physical-Weight'],\n",
    "        'SMM_Height': df['BIA-BIA_SMM'] / df['Physical-Height'],\n",
    "        'Muscle_to_Fat': df['BIA-BIA_SMM'] / df['BIA-BIA_FMI'],\n",
    "        'Hydration_Status': df['BIA-BIA_TBW'] / df['Physical-Weight'],\n",
    "        'ICW_TBW': df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    })\n",
    "\n",
    "    # Concatenate new features with the original DataFrame\n",
    "    df = pd.concat([df, new_features], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_process_data(directory):\n",
    "    files = os.listdir(directory)\n",
    "    all_stats = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(pd.read_parquet, os.path.join(directory, file, 'part-0.parquet')) for file in files]\n",
    "        for future in tqdm(futures):\n",
    "            data = future.result()\n",
    "            if 'step' in data.columns:\n",
    "                data.drop('step', axis=1, inplace=True)\n",
    "\n",
    "            # Calculate summary statistics\n",
    "            stats = data.describe().values.reshape(-1)\n",
    "            all_stats.append(stats)\n",
    "\n",
    "    # Create a DataFrame for summary statistics\n",
    "    stat_columns = [f\"stat_{i}\" for i in range(len(all_stats[0]))]\n",
    "    summary_df = pd.DataFrame(all_stats, columns=stat_columns)\n",
    "    summary_df['id'] = [file.split('=')[1] for file in files]  # Extract 'id' from filenames\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "class SimpleAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(SimpleAutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim * 2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim * 2, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "def train_autoencoder(data, encoding_dim=10, epochs=20, batch_size=16):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    tensor_data = torch.FloatTensor(scaled_data)\n",
    "\n",
    "    autoencoder = SimpleAutoEncoder(input_dim=tensor_data.shape[1], encoding_dim=encoding_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(tensor_data), batch_size):\n",
    "            batch = tensor_data[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(autoencoder(batch), batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(tensor_data).numpy()\n",
    "    \n",
    "    return pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "def impute_missing_values(data, season_columns, season_mapping):\n",
    "    # Encode Seasons\n",
    "    data[season_columns] = data[season_columns].map(lambda x: season_mapping.get(x, x))\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'float32', 'int64']).columns\n",
    "    \n",
    "    # Scale numeric features for KNN imputation\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = data.copy()\n",
    "    data_scaled[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    # Initialize the imputer and apply it only on numeric columns with missing values\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_numeric_data = imputer.fit_transform(data_scaled[numeric_cols])\n",
    "    imputed_scaled_df = pd.DataFrame(imputed_numeric_data, columns=numeric_cols)\n",
    "    \n",
    "    # Invert scaling to original scale for imputed numeric columns\n",
    "    imputed_data = data.copy()\n",
    "    imputed_data[numeric_cols] = scaler.inverse_transform(imputed_scaled_df)\n",
    "    \n",
    "    # Clip and convert 'sii' to integers\n",
    "    if 'sii' in imputed_data.columns:\n",
    "        imputed_data['sii'] = imputed_data['sii'].clip(lower=0, upper=3).round().astype(int)\n",
    "    \n",
    "    # Ensure other columns remain intact\n",
    "    for col in imputed_data.columns:\n",
    "        if col not in numeric_cols:\n",
    "            imputed_data[col] = data[col]\n",
    "    \n",
    "    # Convert season columns to integers\n",
    "    imputed_data[season_columns] = imputed_data[season_columns].clip(lower=1, upper=4).round().astype(int)\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "# Define QWK calculation function\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "# Function to apply threshold-based rounding to predictions\n",
    "def threshold_rounder(predictions, thresholds):\n",
    "    return np.where(predictions < thresholds[0], 0,\n",
    "                    np.where(predictions < thresholds[1], 1,\n",
    "                             np.where(predictions < thresholds[2], 2, 3)))\n",
    "\n",
    "# Threshold optimization to maximize QWK\n",
    "def optimize_qwk_thresholds(predictions, y_true):\n",
    "    def evaluate_thresholds(thresholds):\n",
    "        rounded_preds = threshold_rounder(predictions, thresholds)\n",
    "        return -quadratic_weighted_kappa(y_true, rounded_preds)\n",
    "    \n",
    "    # Optimize using the Nelder-Mead method\n",
    "    result = minimize(evaluate_thresholds, x0=[0.5, 1.5, 2.5], method='Nelder-Mead')\n",
    "    return result.x if result.success else [0.5, 1.5, 2.5]\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4cf9c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:10:59.161515Z",
     "iopub.status.busy": "2024-11-15T13:10:59.161054Z",
     "iopub.status.idle": "2024-11-15T13:10:59.166301Z",
     "shell.execute_reply": "2024-11-15T13:10:59.165024Z"
    },
    "papermill": {
     "duration": 0.015404,
     "end_time": "2024-11-15T13:10:59.168812",
     "exception": false,
     "start_time": "2024-11-15T13:10:59.153408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7c6977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:10:59.183567Z",
     "iopub.status.busy": "2024-11-15T13:10:59.183143Z",
     "iopub.status.idle": "2024-11-15T13:10:59.201153Z",
     "shell.execute_reply": "2024-11-15T13:10:59.199666Z"
    },
    "papermill": {
     "duration": 0.0287,
     "end_time": "2024-11-15T13:10:59.203825",
     "exception": false,
     "start_time": "2024-11-15T13:10:59.175125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "def train_and_evaluate(train, test, model):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    # Arrays to store out-of-fold predictions and test predictions\n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test), n_splits))\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(skf.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Standardize the features using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        test_scaled = scaler.transform(test)\n",
    "        \n",
    "        # Fit the model on the scaled training data\n",
    "        model = clone(model)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "\n",
    "        test_preds[:, fold] = model.predict(test)\n",
    "\n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK: {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK: {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "    oof_tuned = threshold_rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"Optimized QWK SCORE: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    # Prepare submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample_submission['id'],  # Use 'id' from sample submission\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6912a89b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:10:59.218490Z",
     "iopub.status.busy": "2024-11-15T13:10:59.218012Z",
     "iopub.status.idle": "2024-11-15T13:13:56.532163Z",
     "shell.execute_reply": "2024-11-15T13:13:56.530543Z"
    },
    "papermill": {
     "duration": 177.325021,
     "end_time": "2024-11-15T13:13:56.535251",
     "exception": false,
     "start_time": "2024-11-15T13:10:59.210230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [02:31<00:00,  6.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 1.4662\n",
      "Epoch 10/100, Loss: 1.3984\n",
      "Epoch 15/100, Loss: 1.3948\n",
      "Epoch 20/100, Loss: 1.3920\n",
      "Epoch 25/100, Loss: 1.3885\n",
      "Epoch 30/100, Loss: 1.3863\n",
      "Epoch 35/100, Loss: 1.3864\n",
      "Epoch 40/100, Loss: 1.3870\n",
      "Epoch 45/100, Loss: 1.3790\n",
      "Epoch 50/100, Loss: 1.3753\n",
      "Epoch 55/100, Loss: 1.3751\n",
      "Epoch 60/100, Loss: 1.3688\n",
      "Epoch 65/100, Loss: 1.3739\n",
      "Epoch 70/100, Loss: 1.3681\n",
      "Epoch 75/100, Loss: 1.3650\n",
      "Epoch 80/100, Loss: 1.3630\n",
      "Epoch 85/100, Loss: 1.3655\n",
      "Epoch 90/100, Loss: 1.3634\n",
      "Epoch 95/100, Loss: 1.3643\n",
      "Epoch 100/100, Loss: 1.3621\n",
      "Epoch 5/100, Loss: 1.0808\n",
      "Epoch 10/100, Loss: 1.0187\n",
      "Epoch 15/100, Loss: 0.8837\n",
      "Epoch 20/100, Loss: 0.6916\n",
      "Epoch 25/100, Loss: 0.5345\n",
      "Epoch 30/100, Loss: 0.4522\n",
      "Epoch 35/100, Loss: 0.4298\n",
      "Epoch 40/100, Loss: 0.4274\n",
      "Epoch 45/100, Loss: 0.4271\n",
      "Epoch 50/100, Loss: 0.4271\n",
      "Epoch 55/100, Loss: 0.4271\n",
      "Epoch 60/100, Loss: 0.4271\n",
      "Epoch 65/100, Loss: 0.4271\n",
      "Epoch 70/100, Loss: 0.4271\n",
      "Epoch 75/100, Loss: 0.4271\n",
      "Epoch 80/100, Loss: 0.4271\n",
      "Epoch 85/100, Loss: 0.4271\n",
      "Epoch 90/100, Loss: 0.4271\n",
      "Epoch 95/100, Loss: 0.4271\n",
      "Epoch 100/100, Loss: 0.4271\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "train_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "# Load actigraphy time series data\n",
    "train_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "# Autoencode Data\n",
    "train_ts_encoded = train_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
    "test_ts_encoded = train_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "# Add 'id' back to the encoded DataFrame\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "# Merge Data\n",
    "train = pd.merge(train_og, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test_og, test_ts_encoded, how=\"left\", on='id')\n",
    "\n",
    "# Impute Missing Data\n",
    "# Define season mapping\n",
    "season_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\n",
    "# For the train set\n",
    "season_columns_train = [col for col in train.columns if 'Season' in col]\n",
    "train_imputed = impute_missing_values(train, season_columns_train, season_mapping)\n",
    "# For the test set\n",
    "season_columns_test = [col for col in test.columns if 'Season' in col]\n",
    "test_imputed = impute_missing_values(test, season_columns_test, season_mapping)\n",
    "\n",
    "# Perform feature engineering\n",
    "train_imputed = feature_engineering(train_imputed)\n",
    "train_imputed.dropna(thresh=1, axis=0, inplace=True)\n",
    "train_imputed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "test_imputed = feature_engineering(test_imputed)\n",
    "\n",
    "# Get the columns from both DataFrames\n",
    "train_cols = set(train_og.columns)\n",
    "test_cols = set(test_og.columns)\n",
    "\n",
    "# Find common columns\n",
    "common_cols = train_cols.intersection(test_cols)\n",
    "featuresCols = [col for col in common_cols if col != 'id']\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "test_imputed = test_imputed[featuresCols]\n",
    "featuresCols.append('sii')\n",
    "train_imputed = train_imputed[featuresCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f7886cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:13:56.704972Z",
     "iopub.status.busy": "2024-11-15T13:13:56.704123Z",
     "iopub.status.idle": "2024-11-15T13:19:48.700480Z",
     "shell.execute_reply": "2024-11-15T13:19:48.698243Z"
    },
    "papermill": {
     "duration": 352.08597,
     "end_time": "2024-11-15T13:19:48.705269",
     "exception": false,
     "start_time": "2024-11-15T13:13:56.619299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [05:51<00:00, 70.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK: 0.8501\n",
      "Mean Validation QWK: 0.5074\n",
      "Optimized QWK SCORE: \u001b[36m\u001b[1m 0.567\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    2\n",
       "2   00105258    2\n",
       "3   00115b9f    2\n",
       "4   0016bb22    2\n",
       "5   001f3379    2\n",
       "6   0038ba98    2\n",
       "7   0068a485    2\n",
       "8   0069fbed    2\n",
       "9   0083e397    2\n",
       "10  0087dd65    2\n",
       "11  00abe655    2\n",
       "12  00ae59c9    2\n",
       "13  00af6387    2\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    2\n",
       "17  00d9913d    2\n",
       "18  00e6167c    2\n",
       "19  00ebc35d    2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0,\n",
    "    'reg_alpha': 1,\n",
    "    'reg_lambda': 5,\n",
    "    'random_state': 42\n",
    "}\n",
    "cat_params = {\n",
    "    'iterations': 200,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 10,\n",
    "    'subsample': 0.8,\n",
    "    'rsm': 0.8,\n",
    "    'border_count': 32,\n",
    "    'random_state': 42,\n",
    "    'silent': True\n",
    "}\n",
    "rf_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'bootstrap': True,\n",
    "    'random_state': 42\n",
    "}\n",
    "gb_params = {\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'subsample': 1.0,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "xgb_model = XGBRegressor(**xgb_params)\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "gb_model = GradientBoostingRegressor(**gb_params)\n",
    "\n",
    "# Initialize the voting regressor ensemble\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('xgboost', xgb_model),\n",
    "    ('catboost', cat_model),\n",
    "    ('gb', gb_model)\n",
    "])\n",
    "\n",
    "submission1 = train_and_evaluate(train_imputed, test_imputed, voting_regressor)\n",
    "\n",
    "# Display the submission DataFrame\n",
    "submission1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e388e",
   "metadata": {
    "papermill": {
     "duration": 0.084856,
     "end_time": "2024-11-15T13:19:48.905817",
     "exception": false,
     "start_time": "2024-11-15T13:19:48.820961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## No autoencoder. Don't handle missing numerical values. Models = lightgbm, xgboost & catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c657636f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:19:49.082936Z",
     "iopub.status.busy": "2024-11-15T13:19:49.082253Z",
     "iopub.status.idle": "2024-11-15T13:23:59.789490Z",
     "shell.execute_reply": "2024-11-15T13:23:59.788181Z"
    },
    "papermill": {
     "duration": 250.8009,
     "end_time": "2024-11-15T13:23:59.793986",
     "exception": false,
     "start_time": "2024-11-15T13:19:48.993086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [01:34<00:00, 18.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK: 0.7591\n",
      "Mean Validation QWK: 0.3917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized QWK SCORE: \u001b[36m\u001b[1m 0.463\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    0\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_and_evaluate(train, test, model):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test)\n",
    "\n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK: {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK: {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"Optimized QWK SCORE: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample_submission['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "    \n",
    "# Load Data\n",
    "\n",
    "train_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "# Load actigraphy time series data\n",
    "train_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train_og, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test_og, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1) \n",
    "\n",
    "# Get the columns from both DataFrames\n",
    "train_cols = set(train_og.columns)\n",
    "test_cols = set(test_og.columns)\n",
    "\n",
    "# Find common columns\n",
    "common_cols = train_cols.intersection(test_cols)\n",
    "featuresCols = [col for col in common_cols if col != 'id']\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "test = test[featuresCols]\n",
    "\n",
    "featuresCols.append('sii')\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "# Model hyperparameters (keeping your original hyperparameters)\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01  # Increased from 2.68e-06\n",
    "}\n",
    "\n",
    "# Model parameters for CatBoost\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': 42,\n",
    "    'cat_features': cat_c,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10  # Increase this value\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "Light = LGBMRegressor(**Params, random_state=42, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "# Initialize the voting regressor ensemble\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "submission2 = train_and_evaluate(train, test, voting_regressor)\n",
    "\n",
    "# Display the submission DataFrame\n",
    "submission2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27428e10",
   "metadata": {
    "papermill": {
     "duration": 0.086368,
     "end_time": "2024-11-15T13:23:59.965952",
     "exception": false,
     "start_time": "2024-11-15T13:23:59.879584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## No autoencoder. Use imputer = SimpleImputer(strategy='median'). Use more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cad56c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:24:00.139963Z",
     "iopub.status.busy": "2024-11-15T13:24:00.139273Z",
     "iopub.status.idle": "2024-11-15T13:29:04.561167Z",
     "shell.execute_reply": "2024-11-15T13:29:04.559313Z"
    },
    "papermill": {
     "duration": 304.513059,
     "end_time": "2024-11-15T13:29:04.565709",
     "exception": false,
     "start_time": "2024-11-15T13:24:00.052650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [02:38<00:00, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK: 0.9161\n",
      "Mean Validation QWK: 0.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized QWK SCORE: \u001b[36m\u001b[1m 0.452\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    0\n",
       "10  0087dd65    1\n",
       "11  00abe655    0\n",
       "12  00ae59c9    2\n",
       "13  00af6387    2\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "train_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "# Load actigraphy time series data\n",
    "train_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train_og, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test_og, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1) \n",
    "\n",
    "# Get the columns from both DataFrames\n",
    "train_cols = set(train_og.columns)\n",
    "test_cols = set(test_og.columns)\n",
    "\n",
    "# Find common columns\n",
    "common_cols = train_cols.intersection(test_cols)\n",
    "featuresCols = [col for col in common_cols if col != 'id']\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "test = test[featuresCols]\n",
    "\n",
    "featuresCols.append('sii')\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "    \n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "submission3 = train_and_evaluate(train, test, ensemble)\n",
    "\n",
    "# Display the submission DataFrame\n",
    "submission3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074383c9",
   "metadata": {
    "papermill": {
     "duration": 0.088967,
     "end_time": "2024-11-15T13:29:04.742139",
     "exception": false,
     "start_time": "2024-11-15T13:29:04.653172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TPOT to find best parameters. Stacked Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5183553b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:29:04.925029Z",
     "iopub.status.busy": "2024-11-15T13:29:04.924270Z",
     "iopub.status.idle": "2024-11-15T13:54:47.114315Z",
     "shell.execute_reply": "2024-11-15T13:54:47.112768Z"
    },
    "papermill": {
     "duration": 1542.284125,
     "end_time": "2024-11-15T13:54:47.118552",
     "exception": false,
     "start_time": "2024-11-15T13:29:04.834427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [22:46<00:00, 273.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK ---> 0.9671\n",
      "Mean Validation QWK ---> 0.5382\n",
      "Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.579\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    2\n",
       "2   00105258    2\n",
       "3   00115b9f    2\n",
       "4   0016bb22    2\n",
       "5   001f3379    2\n",
       "6   0038ba98    2\n",
       "7   0068a485    2\n",
       "8   0069fbed    2\n",
       "9   0083e397    2\n",
       "10  0087dd65    2\n",
       "11  00abe655    2\n",
       "12  00ae59c9    2\n",
       "13  00af6387    2\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    2\n",
       "17  00d9913d    2\n",
       "18  00e6167c    2\n",
       "19  00ebc35d    2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "train_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "# Load actigraphy time series data\n",
    "train_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "# Autoencode Data\n",
    "train_ts_encoded = train_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
    "test_ts_encoded = train_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "\n",
    "# Add 'id' back to the encoded DataFrame\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "# Merge Data\n",
    "train = pd.merge(train_og, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test_og, test_ts_encoded, how=\"left\", on='id')\n",
    "\n",
    "# Impute Missing Data\n",
    "# Define season mapping\n",
    "season_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\n",
    "\n",
    "# For the train set\n",
    "season_columns_train = [col for col in train.columns if 'Season' in col]\n",
    "train_imputed = impute_missing_values(train, season_columns_train, season_mapping)\n",
    "\n",
    "# For the test set\n",
    "season_columns_test = [col for col in test.columns if 'Season' in col]\n",
    "test_imputed = impute_missing_values(test, season_columns_test, season_mapping)\n",
    "\n",
    "# Perform feature engineering\n",
    "train_imputed = feature_engineering(train_imputed)\n",
    "train_imputed.dropna(thresh=1, axis=0, inplace=True)\n",
    "train_imputed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "test_imputed = feature_engineering(test_imputed)\n",
    "\n",
    "# Get the columns from both DataFrames\n",
    "train_cols = set(train_og.columns)\n",
    "test_cols = set(test_og.columns)\n",
    "\n",
    "# Find common columns\n",
    "common_cols = train_cols.intersection(test_cols)\n",
    "featuresCols = [col for col in common_cols if col != 'id']\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "test_imputed = test_imputed[featuresCols]\n",
    "featuresCols.append('sii')\n",
    "train_imputed = train_imputed[featuresCols]\n",
    "\n",
    "def train_and_evaluate(train, test, models, use_stacking=False, final_estimator_class=None):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test), n_splits))\n",
    "\n",
    "    # Set up cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(skf.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Standardize the features using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        test_scaled = scaler.transform(test)\n",
    "\n",
    "        if use_stacking:\n",
    "            # Initialize the stacked regressor with the specified final estimator\n",
    "            stacked_regressor = StackingRegressor(\n",
    "                estimators=[(model_name, model_class()) for model_class, model_name in models],\n",
    "                final_estimator=final_estimator_class(),\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model = stacked_regressor\n",
    "\n",
    "        else:\n",
    "            # Initialize the voting regressor with the specified models\n",
    "            voting_regressor = VotingRegressor(\n",
    "                estimators=[(model_name, model_class()) for model_class, model_name in models]\n",
    "            )\n",
    "            model = voting_regressor\n",
    "\n",
    "        # Fit the model on the scaled training data\n",
    "        model = clone(model)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Generate predictions for the validation\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test)\n",
    "\n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK ---> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")  \n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    # Prepare submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample_submission['id'],  # Use 'id' from sample submission\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n",
    "models = [\n",
    "    (lambda: RandomForestRegressor(), 'Random Forest'),\n",
    "    (lambda: XGBRegressor(verbosity=0), 'XGBoost'),\n",
    "    (lambda: CatBoostRegressor(verbose=0), 'CatBoost'),\n",
    "    (lambda: GradientBoostingRegressor(), 'Gradient Boosting')\n",
    "]\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Stacking Regressor\n",
    "\n",
    "submission4 = train_and_evaluate(\n",
    "    train_imputed, \n",
    "    test_imputed, \n",
    "    models, \n",
    "    use_stacking=True,\n",
    "    final_estimator_class=GradientBoostingRegressor\n",
    ")\n",
    "\n",
    "submission4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36917084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:54:47.295594Z",
     "iopub.status.busy": "2024-11-15T13:54:47.295062Z",
     "iopub.status.idle": "2024-11-15T13:54:47.344244Z",
     "shell.execute_reply": "2024-11-15T13:54:47.342946Z"
    },
    "papermill": {
     "duration": 0.141229,
     "end_time": "2024-11-15T13:54:47.347045",
     "exception": false,
     "start_time": "2024-11-15T13:54:47.205816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting completed and saved to 'Submission.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    0\n",
       "10  0087dd65    2\n",
       "11  00abe655    0\n",
       "12  00ae59c9    2\n",
       "13  00af6387    2\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1 = submission1\n",
    "sub2 = submission2\n",
    "sub3 = submission3\n",
    "sub4 = submission4\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "sub4 = sub4.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii'],\n",
    "    'sii_4': sub4['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3', 'sii_4']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Submission.csv'\")\n",
    "final_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2660.966478,
   "end_time": "2024-11-15T13:54:51.583995",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-15T13:10:30.617517",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import KNNImputer, IterativeImputer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n                              VotingClassifier, VotingRegressor, GradientBoostingClassifier,\n                              GradientBoostingRegressor, AdaBoostClassifier)\nfrom sklearn.metrics import (classification_report, confusion_matrix, mean_squared_error,\n                             accuracy_score, cohen_kappa_score, r2_score, make_scorer)\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeRegressor \nfrom scipy.optimize import minimize\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-11-04T21:18:23.545517Z","iopub.execute_input":"2024-11-04T21:18:23.545985Z","iopub.status.idle":"2024-11-04T21:18:23.557497Z","shell.execute_reply.started":"2024-11-04T21:18:23.545942Z","shell.execute_reply":"2024-11-04T21:18:23.555979Z"},"trusted":true},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"# Progress Report #1","metadata":{}},{"cell_type":"markdown","source":"# Importing Data","metadata":{}},{"cell_type":"code","source":"# Load Data\n\ntrain_df = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_df = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\ndata_dict = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/data_dictionary.csv')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:45.562960Z","iopub.execute_input":"2024-11-04T20:52:45.563657Z","iopub.status.idle":"2024-11-04T20:52:45.670933Z","shell.execute_reply.started":"2024-11-04T20:52:45.563613Z","shell.execute_reply":"2024-11-04T20:52:45.669782Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"# Get Statistical details\n\ntrain_df.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:45.672319Z","iopub.execute_input":"2024-11-04T20:52:45.672661Z","iopub.status.idle":"2024-11-04T20:52:45.855006Z","shell.execute_reply.started":"2024-11-04T20:52:45.672627Z","shell.execute_reply":"2024-11-04T20:52:45.853831Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:45.856393Z","iopub.execute_input":"2024-11-04T20:52:45.856770Z","iopub.status.idle":"2024-11-04T20:52:45.892270Z","shell.execute_reply.started":"2024-11-04T20:52:45.856731Z","shell.execute_reply":"2024-11-04T20:52:45.890786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:52:45.896271Z","iopub.execute_input":"2024-11-04T20:52:45.896850Z","iopub.status.idle":"2024-11-04T20:52:45.913241Z","shell.execute_reply.started":"2024-11-04T20:52:45.896794Z","shell.execute_reply":"2024-11-04T20:52:45.911722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dict.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:45.914565Z","iopub.execute_input":"2024-11-04T20:52:45.914916Z","iopub.status.idle":"2024-11-04T20:52:45.934527Z","shell.execute_reply.started":"2024-11-04T20:52:45.914882Z","shell.execute_reply":"2024-11-04T20:52:45.933245Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Check the distribution of the target variable 'sii'\nclass_distribution = train_df['sii'].value_counts().sort_index()\nprint(\"Class Distribution:\\n\", class_distribution)\n\n# Set the style for the plot\nsns.set(style=\"whitegrid\")\n\n# Create a bar plot of class distribution\nplt.figure(figsize=(10, 6))\nsns.barplot(x=class_distribution.index, y=class_distribution.values, palette='viridis', alpha=0.8)\n\n# Set the title and labels\nplt.title('Class Distribution of Target Variable (sii)', fontsize=16)\nplt.xlabel('Classes', fontsize=14)\nplt.ylabel('Number of Instances', fontsize=14)\nplt.xticks(rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)  # Optional: add gridlines for better readability\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:45.936222Z","iopub.execute_input":"2024-11-04T20:52:45.936690Z","iopub.status.idle":"2024-11-04T20:52:46.320246Z","shell.execute_reply.started":"2024-11-04T20:52:45.936641Z","shell.execute_reply":"2024-11-04T20:52:46.318944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_stats(data, columns):\n    if isinstance(columns, str):\n        columns = [columns]\n\n    stats = []\n    for col in columns:\n        if data[col].dtype in ['object', 'category']:\n            counts = data[col].value_counts(dropna=False, sort=False)\n            percents = data[col].value_counts(normalize=True, dropna=False, sort=False) * 100\n            formatted = counts.astype(str) + ' (' + percents.round(2).astype(str) + '%)'\n            stats_col = pd.DataFrame({'count (%)': formatted})\n            stats.append(stats_col)\n\n        else:\n            stats_col = data[col].describe().to_frame().transpose()\n            stats_col['missing'] = data[col].isnull().sum()\n            stats_col.index.name = col\n            stats.append(stats_col)\n\n    return pd.concat(stats, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:46.321645Z","iopub.execute_input":"2024-11-04T20:52:46.322033Z","iopub.status.idle":"2024-11-04T20:52:46.330759Z","shell.execute_reply.started":"2024-11-04T20:52:46.321992Z","shell.execute_reply":"2024-11-04T20:52:46.329492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Temporary variable to hold the new column\ntemp_data = train_df.copy()\ntemp_data['Age Group'] = pd.cut(\n    temp_data['Basic_Demos-Age'],\n    bins=[4, 12, 18, 22],\n    labels=['Children (5-12)', 'Adolescents (13-18)', 'Adults (19-22)']\n)\n\ncalculate_stats(temp_data, 'Age Group')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:46.332273Z","iopub.execute_input":"2024-11-04T20:52:46.332728Z","iopub.status.idle":"2024-11-04T20:52:46.357426Z","shell.execute_reply.started":"2024-11-04T20:52:46.332679Z","shell.execute_reply":"2024-11-04T20:52:46.356315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sex_counts_renamed = train_df['Basic_Demos-Sex'].replace({0: 'Male', 1: 'Female'}).value_counts()\nsex_counts_renamed","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:46.358830Z","iopub.execute_input":"2024-11-04T20:52:46.359306Z","iopub.status.idle":"2024-11-04T20:52:46.371458Z","shell.execute_reply.started":"2024-11-04T20:52:46.359257Z","shell.execute_reply":"2024-11-04T20:52:46.370244Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create the boxplot\nfig, ax = plt.subplots(figsize=(5, 5))\n\n# Create the boxplot for SII by Age without hue\nsns.boxplot(y=train_df['Basic_Demos-Age'], x=train_df['sii'], ax=ax, palette=\"Set3\")\n\n# Set the title and labels\nax.set_title('SII by Age')  # Set the title on the ax object\nax.set_ylabel('Age')  # Set the y-axis label\nax.set_xlabel('SII')  # Set the x-axis label\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:46.373030Z","iopub.execute_input":"2024-11-04T20:52:46.373514Z","iopub.status.idle":"2024-11-04T20:52:46.690490Z","shell.execute_reply.started":"2024-11-04T20:52:46.373463Z","shell.execute_reply":"2024-11-04T20:52:46.689191Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the boxplot\n\nfig, ax = plt.subplots(figsize=(5, 5)) \nsns.boxplot(y=train_df['Physical-BMI'], x=train_df['sii'], ax=ax, palette=\"Set3\")  # Pass ax directly\n\n# Set the title and labels\nax.set_title('SII by Physical BMI')  \nax.set_ylabel('BMI')  # Set the y-axis label\nax.set_xlabel('SII')  # Set the x-axis label\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:46.692284Z","iopub.execute_input":"2024-11-04T20:52:46.692646Z","iopub.status.idle":"2024-11-04T20:52:46.988152Z","shell.execute_reply.started":"2024-11-04T20:52:46.692609Z","shell.execute_reply":"2024-11-04T20:52:46.986888Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"common_columns = train_df.columns.intersection(test_df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:52:46.989563Z","iopub.execute_input":"2024-11-04T20:52:46.989949Z","iopub.status.idle":"2024-11-04T20:52:46.995706Z","shell.execute_reply.started":"2024-11-04T20:52:46.989909Z","shell.execute_reply":"2024-11-04T20:52:46.994441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a copy of the original train_df for reference\ntrain_original = train_df.copy()\n\n# Filter train_df to keep only the common columns plus the 'sii' column\ntrain_df = train_df[common_columns.union(['sii'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:52:47.000446Z","iopub.execute_input":"2024-11-04T20:52:47.000872Z","iopub.status.idle":"2024-11-04T20:52:47.014659Z","shell.execute_reply.started":"2024-11-04T20:52:47.000831Z","shell.execute_reply":"2024-11-04T20:52:47.013219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:52:47.016620Z","iopub.execute_input":"2024-11-04T20:52:47.017173Z","iopub.status.idle":"2024-11-04T20:52:47.055411Z","shell.execute_reply.started":"2024-11-04T20:52:47.017111Z","shell.execute_reply":"2024-11-04T20:52:47.054237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check for Duplicate Data","metadata":{}},{"cell_type":"code","source":"train_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.056820Z","iopub.execute_input":"2024-11-04T20:52:47.057285Z","iopub.status.idle":"2024-11-04T20:52:47.082965Z","shell.execute_reply.started":"2024-11-04T20:52:47.057234Z","shell.execute_reply":"2024-11-04T20:52:47.081813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are no duplicates in this data.","metadata":{}},{"cell_type":"markdown","source":"## Missing Values","metadata":{}},{"cell_type":"code","source":"train_df.isnull()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.084312Z","iopub.execute_input":"2024-11-04T20:52:47.084676Z","iopub.status.idle":"2024-11-04T20:52:47.115002Z","shell.execute_reply.started":"2024-11-04T20:52:47.084641Z","shell.execute_reply":"2024-11-04T20:52:47.113761Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.116430Z","iopub.execute_input":"2024-11-04T20:52:47.116890Z","iopub.status.idle":"2024-11-04T20:52:47.131110Z","shell.execute_reply.started":"2024-11-04T20:52:47.116841Z","shell.execute_reply":"2024-11-04T20:52:47.129872Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values by columns\n\ndef check_missing_values(row):\n\n    \"\"\" functions that check and verifies if there are missing values in dataframe \"\"\"\n\n    counter = 0\n    for element in row:\n        if element == True:\n            counter+=1\n\n    return (\"The amount of missing records is: \", counter)\n\ntrain_df.isnull().apply(lambda x: check_missing_values(x))","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.132703Z","iopub.execute_input":"2024-11-04T20:52:47.133084Z","iopub.status.idle":"2024-11-04T20:52:47.199984Z","shell.execute_reply.started":"2024-11-04T20:52:47.133045Z","shell.execute_reply":"2024-11-04T20:52:47.198820Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values in every record\n\ntrain_df.isnull().apply(lambda x: check_missing_values(x), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.201338Z","iopub.execute_input":"2024-11-04T20:52:47.201729Z","iopub.status.idle":"2024-11-04T20:52:47.279112Z","shell.execute_reply.started":"2024-11-04T20:52:47.201678Z","shell.execute_reply":"2024-11-04T20:52:47.277713Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoding Seasons as Numbers","metadata":{}},{"cell_type":"code","source":"# Check for unique values\n\nprint(train_df['Basic_Demos-Enroll_Season'].unique())","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.280336Z","iopub.execute_input":"2024-11-04T20:52:47.280717Z","iopub.status.idle":"2024-11-04T20:52:47.287962Z","shell.execute_reply.started":"2024-11-04T20:52:47.280680Z","shell.execute_reply":"2024-11-04T20:52:47.286281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display columns containing 'Season'\n\nseason_columns = [col for col in train_df.columns if 'Season' in col]\nprint(\"Columns containing 'Season':\", season_columns)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.289572Z","iopub.execute_input":"2024-11-04T20:52:47.290060Z","iopub.status.idle":"2024-11-04T20:52:47.304341Z","shell.execute_reply.started":"2024-11-04T20:52:47.290005Z","shell.execute_reply":"2024-11-04T20:52:47.303061Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"season_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\n\nfor column in season_columns:\n    train_df[column] = train_df[column].map(season_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.306092Z","iopub.execute_input":"2024-11-04T20:52:47.306499Z","iopub.status.idle":"2024-11-04T20:52:47.328289Z","shell.execute_reply.started":"2024-11-04T20:52:47.306461Z","shell.execute_reply":"2024-11-04T20:52:47.326506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# target\ntrain_df[\"sii\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.329812Z","iopub.execute_input":"2024-11-04T20:52:47.330271Z","iopub.status.idle":"2024-11-04T20:52:47.344115Z","shell.execute_reply.started":"2024-11-04T20:52:47.330215Z","shell.execute_reply":"2024-11-04T20:52:47.342687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predict Missing Values","metadata":{}},{"cell_type":"code","source":"# Impute missing values\n\nimputer = KNNImputer(n_neighbors=5, weights='distance', metric='nan_euclidean')\nnumeric_cols = train_df.select_dtypes(include=['float64', 'int64']).columns\nimputed_data = imputer.fit_transform(train_df[numeric_cols])\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n\n# Convert the 'sii' column back to integers and ensure it stays between 0 and 3\n\nif 'sii' in numeric_cols:\n    train_imputed['sii'] = train_imputed['sii'].clip(lower=0, upper=3).round().astype(int)\n\n# Convert season_columns to integers\n\nfor column in season_columns:\n    if column in train_imputed.columns:\n        train_imputed[column] = train_imputed[column].round().astype(int)\n\n# Retain other columns from the original DataFrame\n\nfor col in train_df.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train_df[col]","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:47.345688Z","iopub.execute_input":"2024-11-04T20:52:47.346094Z","iopub.status.idle":"2024-11-04T20:52:54.186706Z","shell.execute_reply.started":"2024-11-04T20:52:47.346030Z","shell.execute_reply":"2024-11-04T20:52:54.185525Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_imputed.isnull().apply(lambda x: check_missing_values(x))","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:54.188170Z","iopub.execute_input":"2024-11-04T20:52:54.188569Z","iopub.status.idle":"2024-11-04T20:52:54.247217Z","shell.execute_reply.started":"2024-11-04T20:52:54.188530Z","shell.execute_reply":"2024-11-04T20:52:54.245793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering\n\n\n### Add variables\n\nThe dataset contains features related to physical characteristics (eg, BMI, Height, Weight), behavioral aspects (eg, internet usage), and fitness data (eg, endurance time).","metadata":{}},{"cell_type":"code","source":"# Feature Engineering\nimport pandas as pd\n\ndef feature_engineering(df):\n    # Calculate new features\n    new_features = pd.DataFrame({\n        'BMI_Age': df['Physical-BMI'] * df['Basic_Demos-Age'],\n        'Internet_Hours_Age': df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age'],\n        'BMI_Internet_Hours': df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday'],\n        'BFP_BMI': df['BIA-BIA_Fat'] / df['BIA-BIA_BMI'],\n        'FFMI_BFP': df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat'],\n        'FMI_BFP': df['BIA-BIA_FMI'] / df['BIA-BIA_Fat'],\n        'LST_TBW': df['BIA-BIA_LST'] / df['BIA-BIA_TBW'],\n        'BFP_BMR': df['BIA-BIA_Fat'] * df['BIA-BIA_BMR'],\n        'BFP_DEE': df['BIA-BIA_Fat'] * df['BIA-BIA_DEE'],\n        'BMR_Weight': df['BIA-BIA_BMR'] / df['Physical-Weight'],\n        'DEE_Weight': df['BIA-BIA_DEE'] / df['Physical-Weight'],\n        'SMM_Height': df['BIA-BIA_SMM'] / df['Physical-Height'],\n        'Muscle_to_Fat': df['BIA-BIA_SMM'] / df['BIA-BIA_FMI'],\n        'Hydration_Status': df['BIA-BIA_TBW'] / df['Physical-Weight'],\n        'ICW_TBW': df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    })\n\n    # Concatenate new features with the original DataFrame\n    df = pd.concat([df, new_features], axis=1)\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:18:40.487466Z","iopub.execute_input":"2024-11-04T21:18:40.487918Z","iopub.status.idle":"2024-11-04T21:18:40.499558Z","shell.execute_reply.started":"2024-11-04T21:18:40.487877Z","shell.execute_reply":"2024-11-04T21:18:40.497850Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Apply feature engineering and clean data\n\ntrain_imputed = feature_engineering(train_imputed)\ntrain_imputed.dropna(thresh=1, axis=0, inplace=True)\ntrain_imputed.replace([np.inf, -np.inf], 0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:54.260532Z","iopub.execute_input":"2024-11-04T20:52:54.260921Z","iopub.status.idle":"2024-11-04T20:52:54.294336Z","shell.execute_reply.started":"2024-11-04T20:52:54.260883Z","shell.execute_reply":"2024-11-04T20:52:54.293148Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### PCA","metadata":{}},{"cell_type":"code","source":"# PCA Implementation\n\n# Split the dataset into features and target variable\nX = train_imputed.drop('id', axis=1)  # Drop 'id' column from features\ny = train_imputed['sii']  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA\npca = PCA(n_components=0.95)  # Preserve 95% of variance\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\n# Check the shape of the PCA output\nprint(\"Original shape:\", X_train.shape)\nprint(\"Transformed shape after PCA:\", X_train_pca.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:54.295706Z","iopub.execute_input":"2024-11-04T20:52:54.296083Z","iopub.status.idle":"2024-11-04T20:52:54.394577Z","shell.execute_reply.started":"2024-11-04T20:52:54.296044Z","shell.execute_reply":"2024-11-04T20:52:54.393415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ada Model","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:21:34.917447Z","iopub.execute_input":"2024-11-01T14:21:34.919288Z","iopub.status.idle":"2024-11-01T14:21:34.925869Z","shell.execute_reply.started":"2024-11-01T14:21:34.919201Z","shell.execute_reply":"2024-11-01T14:21:34.924821Z"}}},{"cell_type":"code","source":"# Initialize the AdaBoost model\nada_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n\n# Fit the model on PCA-transformed training data\nada_model.fit(X_train_pca, y_train)\n\n# Make predictions on the test set\ny_pred_ada = ada_model.predict(X_test_pca)\n\n# Calculate accuracy\naccuracy_ada = accuracy_score(y_test, y_pred_ada)\nprint(f'AdaBoost Model Accuracy: {accuracy_ada:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:54.396236Z","iopub.execute_input":"2024-11-04T20:52:54.398304Z","iopub.status.idle":"2024-11-04T20:52:56.431593Z","shell.execute_reply.started":"2024-11-04T20:52:54.398243Z","shell.execute_reply":"2024-11-04T20:52:56.430109Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gradient Boost","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:21:36.708224Z","iopub.execute_input":"2024-11-01T14:21:36.708676Z","iopub.status.idle":"2024-11-01T14:21:36.714102Z","shell.execute_reply.started":"2024-11-01T14:21:36.708634Z","shell.execute_reply":"2024-11-01T14:21:36.712835Z"}}},{"cell_type":"code","source":"# Initialize the Gradient Boosting model\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n\n# Fit the model on PCA-transformed training data\ngb_model.fit(X_train_pca, y_train)\n\n# Make predictions on the test set\ny_pred_gb = gb_model.predict(X_test_pca)\n\n# Calculate accuracy\naccuracy_gb = accuracy_score(y_test, y_pred_gb)\nprint(f'Gradient Boosting Model Accuracy: {accuracy_gb:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:52:56.433323Z","iopub.execute_input":"2024-11-04T20:52:56.433725Z","iopub.status.idle":"2024-11-04T20:53:13.234882Z","shell.execute_reply.started":"2024-11-04T20:52:56.433688Z","shell.execute_reply":"2024-11-04T20:53:13.233556Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Random Forest","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:21:51.001130Z","iopub.execute_input":"2024-11-01T14:21:51.001822Z","iopub.status.idle":"2024-11-01T14:21:51.009499Z","shell.execute_reply.started":"2024-11-01T14:21:51.001768Z","shell.execute_reply":"2024-11-01T14:21:51.007427Z"}}},{"cell_type":"code","source":"# Initialize the Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n\n# Fit the model on PCA-transformed training data\nrf_model.fit(X_train_pca, y_train)\n\n# Make predictions on the test set\ny_pred_rf = rf_model.predict(X_test_pca)\n\n# Calculate accuracy\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(f'Random Forest Model Accuracy: {accuracy_rf:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:53:13.236241Z","iopub.execute_input":"2024-11-04T20:53:13.236598Z","iopub.status.idle":"2024-11-04T20:53:14.913563Z","shell.execute_reply.started":"2024-11-04T20:53:13.236561Z","shell.execute_reply":"2024-11-04T20:53:14.912361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the voting classifier with the three models\nvoting_model = VotingClassifier(\n    estimators=[\n        ('rf', rf_model),\n        ('ada', ada_model),\n        ('gb', gb_model)\n    ],\n    voting='soft'  # Use soft voting to consider predicted probabilities\n)\n\n# Fit the voting model on PCA-transformed training data\nvoting_model.fit(X_train_pca, y_train)\n\n# Make predictions on the test set\ny_pred_voting = voting_model.predict(X_test_pca)\n\n# Calculate accuracy\naccuracy_voting = accuracy_score(y_test, y_pred_voting)\nprint(f'Voting Classifier Model Accuracy: {accuracy_voting:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T20:53:14.915292Z","iopub.execute_input":"2024-11-04T20:53:14.915662Z","iopub.status.idle":"2024-11-04T20:53:35.331850Z","shell.execute_reply.started":"2024-11-04T20:53:14.915625Z","shell.execute_reply":"2024-11-04T20:53:35.330484Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Progress Report #2","metadata":{}},{"cell_type":"code","source":"# Load Data\n\ntrain_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:18:46.567155Z","iopub.execute_input":"2024-11-04T21:18:46.567616Z","iopub.status.idle":"2024-11-04T21:18:46.639834Z","shell.execute_reply.started":"2024-11-04T21:18:46.567571Z","shell.execute_reply":"2024-11-04T21:18:46.638222Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"### Add in Actigraphy Time Series Data","metadata":{}},{"cell_type":"code","source":"def load_and_process_data(directory):\n    files = os.listdir(directory)\n    all_stats = []\n\n    with ThreadPoolExecutor() as executor:\n        futures = [executor.submit(pd.read_parquet, os.path.join(directory, file, 'part-0.parquet')) for file in files]\n        for future in tqdm(futures):\n            data = future.result()\n            if 'step' in data.columns:\n                data.drop('step', axis=1, inplace=True)\n\n            # Calculate summary statistics\n            stats = data.describe().values.reshape(-1)\n            all_stats.append(stats)\n\n    # Create a DataFrame for summary statistics\n    stat_columns = [f\"stat_{i}\" for i in range(len(all_stats[0]))]\n    summary_df = pd.DataFrame(all_stats, columns=stat_columns)\n    summary_df['id'] = [file.split('=')[1] for file in files]  # Extract 'id' from filenames\n\n    return summary_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:18:47.868337Z","iopub.execute_input":"2024-11-04T21:18:47.868884Z","iopub.status.idle":"2024-11-04T21:18:47.878910Z","shell.execute_reply.started":"2024-11-04T21:18:47.868784Z","shell.execute_reply":"2024-11-04T21:18:47.877578Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Load actigraphy time series data\ntrain_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:18:50.177718Z","iopub.execute_input":"2024-11-04T21:18:50.178241Z","iopub.status.idle":"2024-11-04T21:21:19.116498Z","shell.execute_reply.started":"2024-11-04T21:18:50.178171Z","shell.execute_reply":"2024-11-04T21:21:19.115040Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 996/996 [02:28<00:00,  6.71it/s]\n100%|██████████| 2/2 [00:00<00:00,  6.55it/s]\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"class SimpleAutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(SimpleAutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim * 2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, encoding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim * 2, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.decoder(self.encoder(x))\n\ndef train_autoencoder(data, encoding_dim=10, epochs=20, batch_size=16):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n    tensor_data = torch.FloatTensor(scaled_data)\n\n    autoencoder = SimpleAutoEncoder(input_dim=tensor_data.shape[1], encoding_dim=encoding_dim)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n\n    for epoch in range(epochs):\n        for i in range(0, len(tensor_data), batch_size):\n            batch = tensor_data[i:i + batch_size]\n            optimizer.zero_grad()\n            loss = criterion(autoencoder(batch), batch)\n            loss.backward()\n            optimizer.step()\n        if (epoch + 1) % 5 == 0:\n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n\n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(tensor_data).numpy()\n    \n    return pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:19.118863Z","iopub.execute_input":"2024-11-04T21:21:19.119328Z","iopub.status.idle":"2024-11-04T21:21:19.135230Z","shell.execute_reply.started":"2024-11-04T21:21:19.119278Z","shell.execute_reply":"2024-11-04T21:21:19.133753Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Autoencode Data\ntrain_ts_encoded = train_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\ntest_ts_encoded = train_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:19.137092Z","iopub.execute_input":"2024-11-04T21:21:19.137534Z","iopub.status.idle":"2024-11-04T21:21:25.604443Z","shell.execute_reply.started":"2024-11-04T21:21:19.137493Z","shell.execute_reply":"2024-11-04T21:21:25.603263Z"}},"outputs":[{"name":"stdout","text":"Epoch 5/100, Loss: 1.4830\nEpoch 10/100, Loss: 1.3856\nEpoch 15/100, Loss: 1.3797\nEpoch 20/100, Loss: 1.3719\nEpoch 25/100, Loss: 1.3707\nEpoch 30/100, Loss: 1.3620\nEpoch 35/100, Loss: 1.3590\nEpoch 40/100, Loss: 1.3573\nEpoch 45/100, Loss: 1.3582\nEpoch 50/100, Loss: 1.3575\nEpoch 55/100, Loss: 1.3595\nEpoch 60/100, Loss: 1.3518\nEpoch 65/100, Loss: 1.3535\nEpoch 70/100, Loss: 1.3515\nEpoch 75/100, Loss: 1.3519\nEpoch 80/100, Loss: 1.3508\nEpoch 85/100, Loss: 1.3529\nEpoch 90/100, Loss: 1.3520\nEpoch 95/100, Loss: 1.3517\nEpoch 100/100, Loss: 1.3499\nEpoch 5/100, Loss: 1.0821\nEpoch 10/100, Loss: 1.0216\nEpoch 15/100, Loss: 0.8897\nEpoch 20/100, Loss: 0.6921\nEpoch 25/100, Loss: 0.5248\nEpoch 30/100, Loss: 0.4460\nEpoch 35/100, Loss: 0.4290\nEpoch 40/100, Loss: 0.4273\nEpoch 45/100, Loss: 0.4271\nEpoch 50/100, Loss: 0.4271\nEpoch 55/100, Loss: 0.4271\nEpoch 60/100, Loss: 0.4271\nEpoch 65/100, Loss: 0.4271\nEpoch 70/100, Loss: 0.4271\nEpoch 75/100, Loss: 0.4271\nEpoch 80/100, Loss: 0.4271\nEpoch 85/100, Loss: 0.4271\nEpoch 90/100, Loss: 0.4271\nEpoch 95/100, Loss: 0.4271\nEpoch 100/100, Loss: 0.4271\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"time_series_cols = train_ts_encoded.columns.tolist()\n# Add 'id' back to the encoded DataFrame\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\ntrain = pd.merge(train_og, train_ts_encoded, how=\"left\", on='id')\ntest = pd.merge(test_og, test_ts_encoded, how=\"left\", on='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:25.607198Z","iopub.execute_input":"2024-11-04T21:21:25.607596Z","iopub.status.idle":"2024-11-04T21:21:25.652899Z","shell.execute_reply.started":"2024-11-04T21:21:25.607554Z","shell.execute_reply":"2024-11-04T21:21:25.651712Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Calculate % of Missing sii\n\n# Count the total number of rows\ntotal_rows = len(train)\n\n# Count the missing values in the 'sii' column\nmissing_sii = train['sii'].isna().sum()\n\n# Calculate the percentage of missing values\nmissing_percentage = (missing_sii / total_rows) * 100\n\nprint(f\"Percentage of missing values in 'sii': {missing_percentage:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:25.655061Z","iopub.execute_input":"2024-11-04T21:21:25.655543Z","iopub.status.idle":"2024-11-04T21:21:25.662368Z","shell.execute_reply.started":"2024-11-04T21:21:25.655500Z","shell.execute_reply":"2024-11-04T21:21:25.661236Z"}},"outputs":[{"name":"stdout","text":"Percentage of missing values in 'sii': 30.91%\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"Since more than 30% of the data is missing, imputation might be a better choice than removing the rows. ","metadata":{}},{"cell_type":"markdown","source":"### Impute Missing Data\n#### Use KNN Imputer \n\nGiven the large dataset and the convergence warning upon MICE, switching from IterativeImputer to a simpler imputer, like KNNImputer, might be beneficial for handling such large datasets. \nKNNImputer can be faster and less likely to run into convergence issues, especially with higher-dimensional data.","metadata":{}},{"cell_type":"code","source":"def impute_missing_values(data, season_columns, season_mapping):\n    # Encode Seasons\n    data[season_columns] = data[season_columns].map(lambda x: season_mapping.get(x, x))\n    \n    # Identify numeric columns\n    numeric_cols = data.select_dtypes(include=['float64', 'float32', 'int64']).columns\n    \n    # Scale numeric features for KNN imputation\n    scaler = StandardScaler()\n    data_scaled = data.copy()\n    data_scaled[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n    \n    # Initialize the imputer and apply it only on numeric columns with missing values\n    imputer = KNNImputer(n_neighbors=5)\n    imputed_numeric_data = imputer.fit_transform(data_scaled[numeric_cols])\n    imputed_scaled_df = pd.DataFrame(imputed_numeric_data, columns=numeric_cols)\n    \n    # Invert scaling to original scale for imputed numeric columns\n    imputed_data = data.copy()\n    imputed_data[numeric_cols] = scaler.inverse_transform(imputed_scaled_df)\n    \n    # Clip and convert 'sii' to integers\n    if 'sii' in imputed_data.columns:\n        imputed_data['sii'] = imputed_data['sii'].clip(lower=0, upper=3).round().astype(int)\n    \n    # Ensure other columns remain intact\n    for col in imputed_data.columns:\n        if col not in numeric_cols:\n            imputed_data[col] = data[col]\n    \n    # Convert season columns to integers\n    imputed_data[season_columns] = imputed_data[season_columns].astype(int)\n    \n    return imputed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:25.664078Z","iopub.execute_input":"2024-11-04T21:21:25.664483Z","iopub.status.idle":"2024-11-04T21:21:25.676597Z","shell.execute_reply.started":"2024-11-04T21:21:25.664443Z","shell.execute_reply":"2024-11-04T21:21:25.674784Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Define season mapping\nseason_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\n\n# For the train set\nseason_columns_train = [col for col in train.columns if 'Season' in col]\ntrain_imputed = impute_missing_values(train, season_columns_train, season_mapping)\n\n# For the test set\nseason_columns_test = [col for col in test.columns if 'Season' in col]\ntest_imputed = impute_missing_values(test, season_columns_test, season_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:25.678809Z","iopub.execute_input":"2024-11-04T21:21:25.679233Z","iopub.status.idle":"2024-11-04T21:21:41.880957Z","shell.execute_reply.started":"2024-11-04T21:21:25.679165Z","shell.execute_reply":"2024-11-04T21:21:41.879220Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# Perform feature engineering\ntrain_imputed = feature_engineering(train_imputed)\ntrain_imputed.dropna(thresh=1, axis=0, inplace=True)\ntrain_imputed.replace([np.inf, -np.inf], 0, inplace=True)\n\ntest_imputed = feature_engineering(test_imputed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:41.882846Z","iopub.execute_input":"2024-11-04T21:21:41.883246Z","iopub.status.idle":"2024-11-04T21:21:41.921024Z","shell.execute_reply.started":"2024-11-04T21:21:41.883206Z","shell.execute_reply":"2024-11-04T21:21:41.919532Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# Get the columns from both DataFrames\ntrain_cols = set(train_og.columns)\ntest_cols = set(test_og.columns)\n\n# Find common columns\ncommon_cols = train_cols.intersection(test_cols)\nfeaturesCols = [col for col in common_cols if col != 'id']\nfeaturesCols += time_series_cols\n\ntest_imputed = test_imputed[featuresCols]\nfeaturesCols.append('sii')\ntrain_imputed = train_imputed[featuresCols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:41.922497Z","iopub.execute_input":"2024-11-04T21:21:41.923090Z","iopub.status.idle":"2024-11-04T21:21:41.934479Z","shell.execute_reply.started":"2024-11-04T21:21:41.923036Z","shell.execute_reply":"2024-11-04T21:21:41.933068Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# Split the dataset into features and target variable\nX = train_imputed.drop('sii', axis=1)\ny = train_imputed['sii']  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:41.938549Z","iopub.execute_input":"2024-11-04T21:21:41.939009Z","iopub.status.idle":"2024-11-04T21:21:41.956508Z","shell.execute_reply.started":"2024-11-04T21:21:41.938967Z","shell.execute_reply":"2024-11-04T21:21:41.955205Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ntest_scaled = scaler.transform(test_imputed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:41.958451Z","iopub.execute_input":"2024-11-04T21:21:41.959196Z","iopub.status.idle":"2024-11-04T21:21:41.989878Z","shell.execute_reply.started":"2024-11-04T21:21:41.959112Z","shell.execute_reply":"2024-11-04T21:21:41.988522Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Apply PCA\npca = PCA(n_components=0.95)  # Preserve 95% of variance\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\ntest_pca = pca.transform(test_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:41.991493Z","iopub.execute_input":"2024-11-04T21:21:41.991933Z","iopub.status.idle":"2024-11-04T21:21:42.116929Z","shell.execute_reply.started":"2024-11-04T21:21:41.991890Z","shell.execute_reply":"2024-11-04T21:21:42.115544Z"}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"The target (sii) has an ordinal nature, with values representing categories: 0 for None, 1 for Mild, 2 for Moderate, and 3 for Severe. \nTherefore, regression is appropriate for this analysis to capture the inherent order in the severity levels.","metadata":{}},{"cell_type":"markdown","source":"### Voting Regressor","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{}},{"cell_type":"code","source":"# XGBoost Hyperparameters\nxgb_params = {\n    'n_estimators': 200,\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'gamma': 0,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': 42\n}\n\n# CatBoost Hyperparameters\ncat_params = {\n    'iterations': 200,\n    'learning_rate': 0.05,\n    'depth': 6,\n    'l2_leaf_reg': 10,\n    'subsample': 0.8,\n    'rsm': 0.8,\n    'border_count': 32,\n    'random_state': 42,\n    'silent': True\n}\n\n# Random Forest Hyperparameters\nrf_params = {\n    'n_estimators': 200,\n    'max_depth': None,\n    'min_samples_split': 2,\n    'min_samples_leaf': 1,\n    'bootstrap': True,\n    'random_state': 42\n}\n\n# Gradient Boosting Hyperparameters\ngb_params = {\n    'n_estimators': 200,\n    'learning_rate': 0.05,\n    'max_depth': 3,\n    'min_samples_split': 2,\n    'min_samples_leaf': 1,\n    'subsample': 1.0,\n    'random_state': 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T21:21:42.118869Z","iopub.execute_input":"2024-11-04T21:21:42.119407Z","iopub.status.idle":"2024-11-04T21:21:42.132753Z","shell.execute_reply.started":"2024-11-04T21:21:42.119354Z","shell.execute_reply":"2024-11-04T21:21:42.131106Z"}},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"### QWK Metric","metadata":{}},{"cell_type":"code","source":"# Define QWK calculation function\ndef quadratic_weighted_kappa(y_true, y_pred, num_classes=4):\n    y_pred = y_pred.round().astype(int).clip(0, num_classes - 1)\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n# Define a function to round predictions based on optimized thresholds\ndef threshold_rounder(predictions, thresholds):\n    return np.where(predictions < thresholds[0], 0,\n                    np.where(predictions < thresholds[1], 1,\n                             np.where(predictions < thresholds[2], 2, 3)))\n\n# Optimization function for QWK thresholds\ndef optimize_qwk_thresholds(predictions, y_true):\n    def evaluate_thresholds(thresholds):\n        rounded_preds = threshold_rounder(predictions, thresholds)\n        return -quadratic_weighted_kappa(y_true, rounded_preds)\n    \n    # Optimize thresholds\n    result = minimize(evaluate_thresholds, x0=[0.5, 1.5, 2.5], method='Nelder-Mead')\n    return result.x if result.success else [0.5, 1.5, 2.5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:56:27.985823Z","iopub.execute_input":"2024-11-04T20:56:27.986241Z","iopub.status.idle":"2024-11-04T20:56:28.001874Z","shell.execute_reply.started":"2024-11-04T20:56:27.986193Z","shell.execute_reply":"2024-11-04T20:56:28.000682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize models with specified hyperparameters\nrf_model = RandomForestRegressor(**rf_params)\nxgb_model = XGBRegressor(**xgb_params)\ncat_model = CatBoostRegressor(**cat_params)\ngb_model = GradientBoostingRegressor(**gb_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:56:28.003397Z","iopub.execute_input":"2024-11-04T20:56:28.003819Z","iopub.status.idle":"2024-11-04T20:56:28.023745Z","shell.execute_reply.started":"2024-11-04T20:56:28.003765Z","shell.execute_reply":"2024-11-04T20:56:28.022415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the voting regressor with optimized thresholds\nvoting_regressor = VotingRegressor(estimators=[\n    ('rf', rf_model),\n    ('xgboost', xgb_model),\n    ('catboost', cat_model),\n    ('gb', gb_model)\n])\n\n# Fit and predict with voting regressor\nvoting_regressor.fit(X_train_scaled, y_train)\ny_pred = voting_regressor.predict(X_test_scaled)\n\n# Optimize thresholds to maximize QWK\noptimal_thresholds = optimize_qwk_thresholds(y_pred, y_test)\ny_pred_rounded = threshold_rounder(y_pred, optimal_thresholds)\n\n# Calculate QWK with optimized thresholds\nqwk_score = quadratic_weighted_kappa(y_test, y_pred_rounded)\nprint(f'Optimized Voting Regressor QWK: {qwk_score:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:56:28.025658Z","iopub.execute_input":"2024-11-04T20:56:28.026285Z","iopub.status.idle":"2024-11-04T20:57:37.785629Z","shell.execute_reply.started":"2024-11-04T20:56:28.026241Z","shell.execute_reply":"2024-11-04T20:57:37.784158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Round predictions to nearest integer as QWK requires discrete categories\ny_pred_rounded = np.round(y_pred).astype(int)\n\n# Calculate the Quadratic Weighted Kappa score\nqwk_score = cohen_kappa_score(y_test, y_pred_rounded, weights='quadratic')\nprint(\"Quadratic Weighted Kappa score:\", qwk_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:57:37.787283Z","iopub.execute_input":"2024-11-04T20:57:37.787828Z","iopub.status.idle":"2024-11-04T20:57:37.798732Z","shell.execute_reply.started":"2024-11-04T20:57:37.787773Z","shell.execute_reply":"2024-11-04T20:57:37.797485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on test data and prepare for submission\nsubmission1 = voting_regressor.predict(test_scaled)\n\n# Convert predictions to integers by rounding\nsubmission1 = submission1.round().astype(int)\n\n# Create the submission DataFrame with 'id' from reloaded test_df and 'sii' as integer type\nsubmission = pd.DataFrame({\n    'id': test_og['id'],  # Use the original 'id' column from the reloaded test data\n    'sii': submission1  # 'sii' is now integer as required\n})\n\n# Save the submission file\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T20:57:37.800455Z","iopub.execute_input":"2024-11-04T20:57:37.800914Z","iopub.status.idle":"2024-11-04T20:57:37.843250Z","shell.execute_reply.started":"2024-11-04T20:57:37.800867Z","shell.execute_reply":"2024-11-04T20:57:37.841669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## trial","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.optimize import minimize\nfrom tqdm import tqdm  # For progress bars\nfrom colorama import Fore, Style  # For colored output (if needed)\n\n# Define constants\nN_SPLITS = 5\nSEED = 42\n\n# Define QWK calculation function\ndef quadratic_weighted_kappa(y_true, y_pred, num_classes=4):\n    y_pred = y_pred.round().astype(int).clip(0, num_classes - 1)\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n# Define a function to round predictions based on optimized thresholds\ndef threshold_rounder(predictions, thresholds):\n    return np.where(predictions < thresholds[0], 0,\n                    np.where(predictions < thresholds[1], 1,\n                             np.where(predictions < thresholds[2], 2, 3)))\n\n# Optimization function for QWK thresholds\ndef optimize_thresholds(y_true, preds):\n    return minimize(lambda th: -quadratic_weighted_kappa(y_true, threshold_rounder(preds, th)), \n                    x0=[0.5, 1.5, 2.5], method='Nelder-Mead').x\n\ndef train_model(model, train, test):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n    test_preds = np.zeros((len(test), N_SPLITS))\n    oof_preds = np.zeros(len(y))\n\n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), desc=\"Training Folds\", total=N_SPLITS)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        model.fit(X_train, y_train)\n        oof_preds[val_idx] = model.predict(X_val)\n        test_preds[:, fold] = model.predict(test)\n\n    # Optimize thresholds for out-of-fold predictions\n    optimized_thresholds = optimize_thresholds(y, oof_preds)\n    oof_preds_rounded = threshold_rounder(oof_preds, optimized_thresholds)\n\n    print(f\"Mean Train QWK --> {quadratic_weighted_kappa(y, oof_preds_rounded):.4f}\")\n    \n    final_test_preds = threshold_rounder(test_preds.mean(axis=1), optimized_thresholds)\n    \n    # Print optimized QWK score\n    optimized_qwk_score = quadratic_weighted_kappa(y, oof_preds_rounded)\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT}{optimized_qwk_score:.3f}{Style.RESET_ALL}\")\n\n    return pd.DataFrame({'id': test['id'], 'sii': final_test_preds})\n\n# Example of usage\n# submission = train_model(your_model, train_data, test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
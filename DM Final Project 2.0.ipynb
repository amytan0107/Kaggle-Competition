{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":560.779378,"end_time":"2024-11-05T09:53:23.177289","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-05T09:44:02.397911","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### DM Final Project Progress Report #2 2.0\nscore - 0.392","metadata":{"papermill":{"duration":0.003868,"end_time":"2024-11-05T09:44:05.332158","exception":false,"start_time":"2024-11-05T09:44:05.328290","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\n\nimport warnings\n\n\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom tqdm import tqdm\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n\n\nfrom sklearn.experimental import enable_iterative_imputer\n\nfrom sklearn.impute import KNNImputer, IterativeImputer\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n\n                              VotingClassifier, VotingRegressor, GradientBoostingClassifier,\n\n                              GradientBoostingRegressor, AdaBoostClassifier)\n\nfrom sklearn.metrics import (classification_report, confusion_matrix, mean_squared_error,\n\n                             accuracy_score, cohen_kappa_score, r2_score, make_scorer)\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.tree import DecisionTreeRegressor \n\nfrom scipy.optimize import minimize\n\n\n\nfrom keras.models import Model\n\nfrom keras.layers import Input, Dense\n\n\n\nfrom xgboost import XGBRegressor\n\nfrom catboost import CatBoostRegressor\n\n\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-05T09:44:05.340189Z","iopub.status.busy":"2024-11-05T09:44:05.339781Z","iopub.status.idle":"2024-11-05T09:44:27.012638Z","shell.execute_reply":"2024-11-05T09:44:27.011477Z"},"papermill":{"duration":21.680292,"end_time":"2024-11-05T09:44:27.015640","exception":false,"start_time":"2024-11-05T09:44:05.335348","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Engineering\n\nimport pandas as pd\n\n\n\ndef feature_engineering(df):\n\n    # Calculate new features\n\n    new_features = pd.DataFrame({\n\n        'BMI_Age': df['Physical-BMI'] * df['Basic_Demos-Age'],\n\n        'Internet_Hours_Age': df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age'],\n\n        'BMI_Internet_Hours': df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday'],\n\n        'BFP_BMI': df['BIA-BIA_Fat'] / df['BIA-BIA_BMI'],\n\n        'FFMI_BFP': df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat'],\n\n        'FMI_BFP': df['BIA-BIA_FMI'] / df['BIA-BIA_Fat'],\n\n        'LST_TBW': df['BIA-BIA_LST'] / df['BIA-BIA_TBW'],\n\n        'BFP_BMR': df['BIA-BIA_Fat'] * df['BIA-BIA_BMR'],\n\n        'BFP_DEE': df['BIA-BIA_Fat'] * df['BIA-BIA_DEE'],\n\n        'BMR_Weight': df['BIA-BIA_BMR'] / df['Physical-Weight'],\n\n        'DEE_Weight': df['BIA-BIA_DEE'] / df['Physical-Weight'],\n\n        'SMM_Height': df['BIA-BIA_SMM'] / df['Physical-Height'],\n\n        'Muscle_to_Fat': df['BIA-BIA_SMM'] / df['BIA-BIA_FMI'],\n\n        'Hydration_Status': df['BIA-BIA_TBW'] / df['Physical-Weight'],\n\n        'ICW_TBW': df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n\n    })\n\n\n\n    # Concatenate new features with the original DataFrame\n\n    df = pd.concat([df, new_features], axis=1)\n\n    \n\n    return df\n\n\n\ndef load_and_process_data(directory):\n\n    files = os.listdir(directory)\n\n    all_stats = []\n\n\n\n    with ThreadPoolExecutor() as executor:\n\n        futures = [executor.submit(pd.read_parquet, os.path.join(directory, file, 'part-0.parquet')) for file in files]\n\n        for future in tqdm(futures):\n\n            data = future.result()\n\n            if 'step' in data.columns:\n\n                data.drop('step', axis=1, inplace=True)\n\n\n\n            # Calculate summary statistics\n\n            stats = data.describe().values.reshape(-1)\n\n            all_stats.append(stats)\n\n\n\n    # Create a DataFrame for summary statistics\n\n    stat_columns = [f\"stat_{i}\" for i in range(len(all_stats[0]))]\n\n    summary_df = pd.DataFrame(all_stats, columns=stat_columns)\n\n    summary_df['id'] = [file.split('=')[1] for file in files]  # Extract 'id' from filenames\n\n\n\n    return summary_df\n\n\n\nclass SimpleAutoEncoder(nn.Module):\n\n    def __init__(self, input_dim, encoding_dim):\n\n        super(SimpleAutoEncoder, self).__init__()\n\n        self.encoder = nn.Sequential(\n\n            nn.Linear(input_dim, encoding_dim * 2),\n\n            nn.ReLU(),\n\n            nn.Linear(encoding_dim * 2, encoding_dim),\n\n            nn.ReLU()\n\n        )\n\n        self.decoder = nn.Sequential(\n\n            nn.Linear(encoding_dim, encoding_dim * 2),\n\n            nn.ReLU(),\n\n            nn.Linear(encoding_dim * 2, input_dim),\n\n            nn.Sigmoid()\n\n        )\n\n\n\n    def forward(self, x):\n\n        return self.decoder(self.encoder(x))\n\n\n\ndef train_autoencoder(data, encoding_dim=10, epochs=20, batch_size=16):\n\n    scaler = StandardScaler()\n\n    scaled_data = scaler.fit_transform(data)\n\n    tensor_data = torch.FloatTensor(scaled_data)\n\n\n\n    autoencoder = SimpleAutoEncoder(input_dim=tensor_data.shape[1], encoding_dim=encoding_dim)\n\n    criterion = nn.MSELoss()\n\n    optimizer = optim.Adam(autoencoder.parameters())\n\n\n\n    for epoch in range(epochs):\n\n        for i in range(0, len(tensor_data), batch_size):\n\n            batch = tensor_data[i:i + batch_size]\n\n            optimizer.zero_grad()\n\n            loss = criterion(autoencoder(batch), batch)\n\n            loss.backward()\n\n            optimizer.step()\n\n        if (epoch + 1) % 5 == 0:\n\n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n\n\n\n    with torch.no_grad():\n\n        encoded_data = autoencoder.encoder(tensor_data).numpy()\n\n    \n\n    return pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n\n    \n\ndef impute_missing_values(data, season_columns, season_mapping):\n\n    # Encode Seasons\n\n    data[season_columns] = data[season_columns].map(lambda x: season_mapping.get(x, x))\n\n    \n\n    # Identify numeric columns\n\n    numeric_cols = data.select_dtypes(include=['float64', 'float32', 'int64']).columns\n\n    \n\n    # Scale numeric features for KNN imputation\n\n    scaler = StandardScaler()\n\n    data_scaled = data.copy()\n\n    data_scaled[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n\n    \n\n    # Initialize the imputer and apply it only on numeric columns with missing values\n\n    imputer = KNNImputer(n_neighbors=5)\n\n    imputed_numeric_data = imputer.fit_transform(data_scaled[numeric_cols])\n\n    imputed_scaled_df = pd.DataFrame(imputed_numeric_data, columns=numeric_cols)\n\n    \n\n    # Invert scaling to original scale for imputed numeric columns\n\n    imputed_data = data.copy()\n\n    imputed_data[numeric_cols] = scaler.inverse_transform(imputed_scaled_df)\n\n    \n\n    # Clip and convert 'sii' to integers\n\n    if 'sii' in imputed_data.columns:\n\n        imputed_data['sii'] = imputed_data['sii'].clip(lower=0, upper=3).round().astype(int)\n\n    \n\n    # Ensure other columns remain intact\n\n    for col in imputed_data.columns:\n\n        if col not in numeric_cols:\n\n            imputed_data[col] = data[col]\n\n    \n\n    # Convert season columns to integers\n\n    imputed_data[season_columns] = imputed_data[season_columns].astype(int)\n\n    \n\n    return imputed_data\n\n\n\n# Define QWK calculation function\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n\n\n# Function to apply threshold-based rounding to predictions\n\ndef threshold_rounder(predictions, thresholds):\n\n    return np.where(predictions < thresholds[0], 0,\n\n                    np.where(predictions < thresholds[1], 1,\n\n                             np.where(predictions < thresholds[2], 2, 3)))\n\n\n\n# Threshold optimization to maximize QWK\n\ndef optimize_qwk_thresholds(predictions, y_true):\n\n    def evaluate_thresholds(thresholds):\n\n        rounded_preds = threshold_rounder(predictions, thresholds)\n\n        return -quadratic_weighted_kappa(y_true, rounded_preds)\n\n    \n\n    # Optimize using the Nelder-Mead method\n\n    result = minimize(evaluate_thresholds, x0=[0.5, 1.5, 2.5], method='Nelder-Mead')\n\n    return result.x if result.success else [0.5, 1.5, 2.5]\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n# Model\n\ndef train_and_evaluate(train, test, sample_submission, n_splits=5, random_state=42):\n\n    X = train.drop(['sii'], axis=1)\n\n    y = train['sii']\n\n    \n\n    # Arrays to store out-of-fold predictions and test predictions\n\n    oof_preds = np.zeros(len(y))\n\n    test_preds = np.zeros((len(test), n_splits))\n\n    \n\n    # Set up cross-validation\n\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n\n        print(f\"Training fold {fold + 1}/{n_splits}...\")\n\n        \n\n        # Split the data into training and validation sets\n\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        \n\n        # Standardize the features using StandardScaler\n\n        scaler = StandardScaler()\n\n        X_train_scaled = scaler.fit_transform(X_train)\n\n        X_val_scaled = scaler.transform(X_val)\n\n        test_scaled = scaler.transform(test)\n\n        \n\n        # Fit the ensemble model on the scaled training data\n\n        voting_regressor.fit(X_train_scaled, y_train)\n\n        \n\n        # Generate predictions for the validation and test sets\n\n        oof_preds[val_idx] = voting_regressor.predict(X_val_scaled)\n\n        test_preds[:, fold] = voting_regressor.predict(test_scaled)\n\n    \n\n    # Optimize thresholds based on out-of-fold predictions\n\n    optimal_thresholds = optimize_qwk_thresholds(oof_preds, y)\n\n    oof_preds_rounded = threshold_rounder(oof_preds, optimal_thresholds)\n\n    \n\n    # Calculate and print optimized QWK score\n\n    qwk_score = quadratic_weighted_kappa(y, oof_preds_rounded)\n\n    print(f\"Optimized QWK: {qwk_score:.4f}\")\n\n    \n\n    # Average test predictions and apply optimized thresholds for final submission\n\n    final_test_preds = threshold_rounder(test_preds.mean(axis=1), optimal_thresholds)\n\n    \n\n    # Prepare submission DataFrame\n\n    submission = pd.DataFrame({\n\n        'id': sample_submission['id'],  # Use 'id' from sample submission\n\n        'sii': final_test_preds.astype(int)  # Round to integer for submission\n\n    })\n\n    \n\n    return submission","metadata":{"execution":{"iopub.execute_input":"2024-11-05T09:44:27.026304Z","iopub.status.busy":"2024-11-05T09:44:27.025546Z","iopub.status.idle":"2024-11-05T09:44:27.058725Z","shell.execute_reply":"2024-11-05T09:44:27.057626Z"},"papermill":{"duration":0.041357,"end_time":"2024-11-05T09:44:27.061292","exception":false,"start_time":"2024-11-05T09:44:27.019935","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Data\n\n\n\ntrain_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n\ntest_og = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n\nsample_submission = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n\n\n# Load actigraphy time series data\n\ntrain_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n\ntest_ts = load_and_process_data(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\n\n\ndf_train = train_ts.drop('id', axis=1)\n\ndf_test = test_ts.drop('id', axis=1)\n\n\n\n# Autoencode Data\n\ntrain_ts_encoded = train_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n\ntest_ts_encoded = train_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\n\n\ntime_series_cols = train_ts_encoded.columns.tolist()\n\n# Add 'id' back to the encoded DataFrame\n\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\n\ntest_ts_encoded['id']=test_ts[\"id\"]\n\n\n\n# Merge Data\n\ntrain = pd.merge(train_og, train_ts_encoded, how=\"left\", on='id')\n\ntest = pd.merge(test_og, test_ts_encoded, how=\"left\", on='id')\n\n\n\n# Impute Missing Data\n\n# Define season mapping\n\nseason_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\n\n# For the train set\n\nseason_columns_train = [col for col in train.columns if 'Season' in col]\n\ntrain_imputed = impute_missing_values(train, season_columns_train, season_mapping)\n\n# For the test set\n\nseason_columns_test = [col for col in test.columns if 'Season' in col]\n\ntest_imputed = impute_missing_values(test, season_columns_test, season_mapping)\n\n\n\n# Perform feature engineering\n\ntrain_imputed = feature_engineering(train_imputed)\n\ntrain_imputed.dropna(thresh=1, axis=0, inplace=True)\n\ntrain_imputed.replace([np.inf, -np.inf], 0, inplace=True)\n\ntest_imputed = feature_engineering(test_imputed)\n\n\n\n# Get the columns from both DataFrames\n\ntrain_cols = set(train_og.columns)\n\ntest_cols = set(test_og.columns)\n\n\n\n# Find common columns\n\ncommon_cols = train_cols.intersection(test_cols)\n\nfeaturesCols = [col for col in common_cols if col != 'id']\n\nfeaturesCols += time_series_cols\n\n\n\ntest_imputed = test_imputed[featuresCols]\n\nfeaturesCols.append('sii')\n\ntrain_imputed = train_imputed[featuresCols]\n\n\n\n# Split the dataset into features and target variable\n\nX = train_imputed.drop('sii', axis=1)\n\ny = train_imputed['sii']  # Target variable\n\n\n\n# Split the dataset into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n# Standardize the features\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\nX_test_scaled = scaler.transform(X_test)\n\ntest_scaled = scaler.transform(test_imputed)\n\n\n\n# Apply PCA\n\npca = PCA(n_components=0.95)  # Preserve 95% of variance\n\nX_train_pca = pca.fit_transform(X_train_scaled)\n\nX_test_pca = pca.transform(X_test_scaled)\n\ntest_pca = pca.transform(test_scaled)","metadata":{"execution":{"iopub.execute_input":"2024-11-05T09:44:27.071371Z","iopub.status.busy":"2024-11-05T09:44:27.070556Z","iopub.status.idle":"2024-11-05T09:47:38.619150Z","shell.execute_reply":"2024-11-05T09:47:38.617519Z"},"papermill":{"duration":191.557159,"end_time":"2024-11-05T09:47:38.622530","exception":false,"start_time":"2024-11-05T09:44:27.065371","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model hyperparameters (keeping your original hyperparameters)\n\nxgb_params = {\n\n    'n_estimators': 200,\n\n    'learning_rate': 0.05,\n\n    'max_depth': 6,\n\n    'min_child_weight': 1,\n\n    'subsample': 0.8,\n\n    'colsample_bytree': 0.8,\n\n    'gamma': 0,\n\n    'reg_alpha': 1,\n\n    'reg_lambda': 5,\n\n    'random_state': 42\n\n}\n\ncat_params = {\n\n    'iterations': 200,\n\n    'learning_rate': 0.05,\n\n    'depth': 6,\n\n    'l2_leaf_reg': 10,\n\n    'subsample': 0.8,\n\n    'rsm': 0.8,\n\n    'border_count': 32,\n\n    'random_state': 42,\n\n    'silent': True\n\n}\n\nrf_params = {\n\n    'n_estimators': 200,\n\n    'max_depth': None,\n\n    'min_samples_split': 2,\n\n    'min_samples_leaf': 1,\n\n    'bootstrap': True,\n\n    'random_state': 42\n\n}\n\ngb_params = {\n\n    'n_estimators': 200,\n\n    'learning_rate': 0.05,\n\n    'max_depth': 3,\n\n    'min_samples_split': 2,\n\n    'min_samples_leaf': 1,\n\n    'subsample': 1.0,\n\n    'random_state': 42\n\n}\n\n\n\n# Initialize the models\n\nrf_model = RandomForestRegressor(**rf_params)\n\nxgb_model = XGBRegressor(**xgb_params)\n\ncat_model = CatBoostRegressor(**cat_params)\n\ngb_model = GradientBoostingRegressor(**gb_params)\n\n\n\n# Initialize the voting regressor ensemble\n\nvoting_regressor = VotingRegressor(estimators=[\n\n    ('rf', rf_model),\n\n    ('xgboost', xgb_model),\n\n    ('catboost', cat_model),\n\n    ('gb', gb_model)\n\n])\n\n\n\n# with scaling can vs above qwk\n\nsubmission = train_and_evaluate(train_imputed, test_imputed, sample_submission)\n\n\n\n# Display the submission DataFrame\n\nsubmission","metadata":{"execution":{"iopub.execute_input":"2024-11-05T09:47:38.798408Z","iopub.status.busy":"2024-11-05T09:47:38.797702Z","iopub.status.idle":"2024-11-05T09:53:18.789723Z","shell.execute_reply":"2024-11-05T09:53:18.788627Z"},"papermill":{"duration":340.10279,"end_time":"2024-11-05T09:53:18.845801","exception":false,"start_time":"2024-11-05T09:47:38.743011","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the submission file\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Submission file created: submission.csv\")\n\nsubmission","metadata":{"execution":{"iopub.execute_input":"2024-11-05T09:53:18.956162Z","iopub.status.busy":"2024-11-05T09:53:18.955732Z","iopub.status.idle":"2024-11-05T09:53:18.972222Z","shell.execute_reply":"2024-11-05T09:53:18.971190Z"},"papermill":{"duration":0.074993,"end_time":"2024-11-05T09:53:18.974597","exception":false,"start_time":"2024-11-05T09:53:18.899604","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}